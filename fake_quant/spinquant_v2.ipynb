{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rotation_utils import random_orthogonal_matrix\n",
    "from hadamard_utils import random_hadamard_matrix, apply_exact_had_to_linear\n",
    "from quant_utils import ActQuantWrapper\n",
    "\n",
    "import utils\n",
    "import model_utils\n",
    "import data_utils\n",
    "import transformers\n",
    "import quant_utils\n",
    "import rotation_utils\n",
    "import gptq_utils\n",
    "import eval_utils\n",
    "import hadamard_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatedEmbedding(nn.Embedding):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        padding_idx=None,\n",
    "        max_norm=None,\n",
    "        norm_type=2.0,\n",
    "        scale_grad_by_freq=False,\n",
    "        sparse=False,\n",
    "        _weight=None,\n",
    "        _freeze=False,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx,\n",
    "                         max_norm, norm_type, scale_grad_by_freq, sparse,\n",
    "                         _weight, _freeze, device, dtype)\n",
    "        \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "            #self.register_parameter(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W = self.weight\n",
    "        \n",
    "        #if W.device != self.Q.device:\n",
    "        #    self.Q = self.Q.to(W.device)\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(W.to(dtype=self.Q.dtype), self.Q.to(W.device)).to(dtype=W.dtype)\n",
    "            #print('emb')\n",
    "            #print(W_.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        #print(x.device, W.device)\n",
    "        \n",
    "        return F.embedding(\n",
    "            x, W_,\n",
    "            self.padding_idx,\n",
    "            self.max_norm,\n",
    "            self.norm_type,\n",
    "            self.scale_grad_by_freq,\n",
    "            self.sparse)\n",
    "\n",
    "class RotatedHead(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "            #self.register_parameter(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        W = self.weight\n",
    "        \n",
    "        #if W.device != self.Q.device:\n",
    "        #    self.Q = self.Q.to(W.device)\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(W.to(dtype=self.Q.dtype), self.Q.to(W.device)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('head')\n",
    "            #print(W_.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_,\n",
    "        )\n",
    "\n",
    "class RotatedLinearIn(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "            #self.register_parameter(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        W = self.weight\n",
    "        \n",
    "        #if W.device != self.Q.device:\n",
    "        #    self.Q = self.Q.to(W.device)\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(W.to(dtype=self.Q.dtype), self.Q.to(W.device)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('linear in')\n",
    "            #print(W_.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_,\n",
    "        )\n",
    "\n",
    "\n",
    "class RotatedOVProj(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Qin=None,\n",
    "        Qout=None,\n",
    "        output=False,\n",
    "        nheads=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Qin is not None:\n",
    "            self.register_buffer(\"Qin\", Qin)\n",
    "            #self.register_parameter(\"Qin\", Qin)\n",
    "        else:\n",
    "            self.Qin = None\n",
    "        \n",
    "        if Qout is not None:\n",
    "            self.register_buffer(\"Qout\", Qout)\n",
    "            #self.register_parameter(\"Qout\", Qout)\n",
    "        else:\n",
    "            self.Qout = None\n",
    "        \n",
    "        self.output = output\n",
    "        self.nheads = nheads\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W = self.weight\n",
    "        \n",
    "        # if W.device != self.Qin.device:\n",
    "        #     self.Qin = self.Qin.to(W.device)\n",
    "        \n",
    "        # if W.device != self.Qout.device:\n",
    "        #     self.Qout = self.Qout.to(W.device)\n",
    "        \n",
    "        if self.Qin is not None:\n",
    "            if self.output:\n",
    "                W_ = torch.matmul(W.to(dtype=self.Qin.dtype), self.Qin.to(W.device)).to(dtype=W.dtype)\n",
    "            else:\n",
    "                W_ = W.to(dtype=self.Qin.dtype).reshape(W.size(0), self.nheads, -1)\n",
    "                W_ = torch.einsum('inh,hj->inj', W_, self.Qin.to(W.device)).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear o', W.grad_fn, W_.grad_fn, self.Qin.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        if self.Qout is not None:\n",
    "            if self.output:\n",
    "                W_ = W_.to(dtype=self.Qout.dtype).reshape(self.nheads, -1, W.size(1))\n",
    "                W_ = torch.einsum('ih,nhj->nij', self.Qout.to(W.device).T, W_).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear v', W.grad_fn, W_.grad_fn, self.Qout.grad_fn)\n",
    "                #print(W_.grad_fn)\n",
    "            else:\n",
    "                W_ = torch.matmul(self.Qout.to(W.device).T, W_.to(dtype=self.Qout.dtype)).to(dtype=W.dtype)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_,\n",
    "        )\n",
    "\n",
    "\n",
    "class RotatedLinearOut(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "            #self.register_parameter(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        \n",
    "        # if W.device != self.Q.device:\n",
    "        #     self.Q = self.Q.to(W.device)\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(self.Q.to(W.device).T, W.to(dtype=self.Q.dtype)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('linear out')\n",
    "            #print(W_.grad_fn)\n",
    "            if b is not None:\n",
    "                b_ = torch.matmul(self.Q.to(W.device).T, b.to(dtype=self.Q.dtype)).to(dtype=b.dtype)\n",
    "            else:\n",
    "                b_ = b\n",
    "        else:\n",
    "            W_ = W\n",
    "            b_ = b\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_, b_\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_embeddings(model, Q):\n",
    "    \n",
    "    original_emb = model.model.embed_tokens\n",
    "    \n",
    "    new_emb = RotatedEmbedding(\n",
    "        original_emb.num_embeddings,\n",
    "        original_emb.embedding_dim,\n",
    "        original_emb.padding_idx,\n",
    "        original_emb.max_norm,\n",
    "        original_emb.norm_type,\n",
    "        original_emb.scale_grad_by_freq,\n",
    "        original_emb.sparse,\n",
    "        original_emb.weight.data,\n",
    "        not original_emb.weight.requires_grad,\n",
    "        original_emb.weight.data.device,\n",
    "        original_emb.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    setattr(model.model, 'embed_tokens', new_emb)\n",
    "\n",
    "\n",
    "def rotate_attention_inputs(layer, Q) -> None:\n",
    "    # Rotate the WQ, WK and WV matrices of the self-attention layer.\n",
    "    for name in ['q_proj', 'k_proj']:#, 'v_proj']:\n",
    "        original_matrix = getattr(layer.self_attn, name)\n",
    "        \n",
    "        new_matrix = RotatedLinearIn(\n",
    "            original_matrix.in_features,\n",
    "            original_matrix.out_features,\n",
    "            original_matrix.bias is not None,\n",
    "            original_matrix.weight.data.device,\n",
    "            original_matrix.weight.data.dtype,\n",
    "            Q\n",
    "        )\n",
    "        \n",
    "        new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "        if original_matrix.bias is not None:\n",
    "            new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "        \n",
    "        setattr(layer.self_attn, name, new_matrix)\n",
    "        del original_matrix\n",
    "\n",
    "\n",
    "def rotate_attention_output(layer, Q) -> None:\n",
    "    # Rotate output matrix of the self-attention layer.\n",
    "    original_matrix = layer.self_attn.o_proj\n",
    "    \n",
    "    new_matrix = RotatedLinearOut(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.self_attn, 'o_proj', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_mlp_input(layer, Q):\n",
    "    # Rotate the MLP input weights.\n",
    "    \n",
    "    for name in ['up_proj', 'gate_proj']:\n",
    "        original_matrix = getattr(layer.mlp, name)\n",
    "        \n",
    "        new_matrix = RotatedLinearIn(\n",
    "            original_matrix.in_features,\n",
    "            original_matrix.out_features,\n",
    "            original_matrix.bias is not None,\n",
    "            original_matrix.weight.data.device,\n",
    "            original_matrix.weight.data.dtype,\n",
    "            Q\n",
    "        )\n",
    "        \n",
    "        new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "        if original_matrix.bias is not None:\n",
    "            new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "        \n",
    "        setattr(layer.mlp, name, new_matrix)\n",
    "        del original_matrix\n",
    "\n",
    "\n",
    "def rotate_mlp_output(layer, Q):\n",
    "    # Rotate the MLP output weights and bias.\n",
    "    original_matrix = layer.mlp.down_proj\n",
    "    \n",
    "    new_matrix = RotatedLinearOut(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.mlp, 'down_proj', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_head(model, Q: torch.Tensor) -> None:\n",
    "    # Rotate the head.\n",
    "    original_matrix = model.lm_head\n",
    "    \n",
    "    new_matrix = RotatedLinearIn(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(model, 'lm_head', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_ov_proj(layer, Q1, Q2, nheads):\n",
    "    #print(nheads)\n",
    "    original_matrix = layer.self_attn.o_proj\n",
    "    \n",
    "    new_matrix = RotatedOVProj(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q2, Q1, False, nheads\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.self_attn, 'o_proj', new_matrix)\n",
    "    del original_matrix\n",
    "    \n",
    "    original_matrix = layer.self_attn.v_proj\n",
    "    \n",
    "    new_matrix = RotatedOVProj(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q1, Q2, True, nheads\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.self_attn, 'v_proj', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_model(model, args):\n",
    "    #q = random_orthogonal_matrix(model.config.hidden_size, utils.DEV).to(dtype=torch.float32)\n",
    "    \n",
    "    config = model.config\n",
    "    num_heads = config.num_attention_heads\n",
    "    model_dim = config.hidden_size\n",
    "    head_dim = model_dim // num_heads\n",
    "    \n",
    "    Q1 = random_hadamard_matrix(model.config.hidden_size, 'cpu').to(dtype=torch.float64).requires_grad_(True)\n",
    "    Q1 = nn.Parameter(Q1, requires_grad=True)\n",
    "    Q2s = []\n",
    "    #q2 = random_hadamard_matrix(head_dim, utils.DEV).to(dtype=torch.float32)\n",
    "    #Q2 = nn.Parameter(q2, requires_grad=True)\n",
    "    \n",
    "    model_type = model_utils.model_type_extractor(model)\n",
    "    rotate_embeddings(model, Q1)\n",
    "    rotate_head(model, Q1)\n",
    "    utils.cleanup_memory()\n",
    "    layers = model_utils.get_transformer_layers(model, \n",
    "                                                model_type=model_type)\n",
    "    \n",
    "    for idx, layer in enumerate(tqdm.tqdm(layers, unit=\"layer\", desc=\"Rotating\")):\n",
    "        Q2 = random_hadamard_matrix(head_dim, layers[idx].self_attn.v_proj.weight.device).to(dtype=torch.float64).requires_grad_(True)#.clone().detach().requires_grad_(True)\n",
    "        #q2 = random_hadamard_matrix(head_dim, utils.DEV).to(dtype=torch.float64)#.clone().detach().requires_grad_(True)\n",
    "        #print(q2.device)\n",
    "        Q2 = nn.Parameter(Q2, requires_grad=True)\n",
    "        rotate_attention_inputs(layers[idx], Q1)\n",
    "        #rotate_attention_output(layers[idx], Q1)\n",
    "        rotate_mlp_input(layers[idx], Q1)\n",
    "        rotate_mlp_output(layers[idx], Q1)\n",
    "        rotate_ov_proj(layers[idx], Q1, Q2, num_heads)\n",
    "        \n",
    "        Q2s.append(Q2)\n",
    "        #print(str(idx) + '-----------')\n",
    "        #print(layer.self_attn.v_proj)\n",
    "        #print(layer.self_attn.o_proj)\n",
    "    return Q1, Q2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "{'a_asym': False,\n",
      " 'a_bits': 4,\n",
      " 'a_clip_ratio': 1.0,\n",
      " 'a_groupsize': -1,\n",
      " 'act_order': False,\n",
      " 'bsz': 1,\n",
      " 'cal_dataset': 'wikitext2',\n",
      " 'capture_layer_io': False,\n",
      " 'distribute': False,\n",
      " 'eval_dataset': 'wikitext2',\n",
      " 'fp32_had': False,\n",
      " 'hf_token': None,\n",
      " 'int8_down_proj': False,\n",
      " 'k_asym': False,\n",
      " 'k_bits': 4,\n",
      " 'k_clip_ratio': 1.0,\n",
      " 'k_groupsize': -1,\n",
      " 'k_pre_rope': False,\n",
      " 'layer_idx': 10,\n",
      " 'lm_eval': False,\n",
      " 'lm_eval_batch_size': 128,\n",
      " 'load_qmodel_path': None,\n",
      " 'model': 'meta-llama/Llama-2-7b-hf',\n",
      " 'nsamples': 128,\n",
      " 'percdamp': 0.01,\n",
      " 'rotate': True,\n",
      " 'rotate_mode': 'hadamard',\n",
      " 'rotation_seed': -1,\n",
      " 'save_name': '20240618_075552',\n",
      " 'save_path': '/ceph/echoi/codes/QuaRot/fake_quant/experiments/meta-llama/Llama-2-7b-hf/20240618_075552',\n",
      " 'save_qmodel_path': None,\n",
      " 'seed': 0,\n",
      " 'tasks': ['piqa',\n",
      "           'hellaswag',\n",
      "           'arc_easy',\n",
      "           'arc_challenge',\n",
      "           'winogrande',\n",
      "           'lambada'],\n",
      " 'v_asym': False,\n",
      " 'v_bits': 4,\n",
      " 'v_clip_ratio': 1.0,\n",
      " 'v_groupsize': -1,\n",
      " 'w_asym': False,\n",
      " 'w_bits': 16,\n",
      " 'w_clip': True,\n",
      " 'w_groupsize': -1,\n",
      " 'w_rtn': False,\n",
      " 'wandb': False,\n",
      " 'wandb_id': None,\n",
      " 'wandb_project': None}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = utils.parser_gen('--model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 16 --w_clip --bsz 1'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.96it/s]\n",
      "---> Loading meta-llama/Llama-2-7b-hf Model with seq_len: 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.set_seed(args.seed)\n",
    "model = model_utils.get_model(args.model, args.hf_token)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU memory (from <module>): 0.00 -> 0.00 GB (0.00 GB)\n"
     ]
    }
   ],
   "source": [
    "rotation_utils.fuse_layer_norms(model)\n",
    "utils.cleanup_memory(verbos=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU memory (from rotate_model): 0.00 -> 0.00 GB (0.00 GB)\n",
      "Rotating: 100%|██████████| 32/32 [00:33<00:00,  1.05s/layer]\n",
      "GPU memory (from <module>): 0.00 -> 0.00 GB (0.00 GB)\n"
     ]
    }
   ],
   "source": [
    "Q1, Q2s = rotate_model(model, args)\n",
    "utils.cleanup_memory(verbos=True)\n",
    "\n",
    "quant_utils.add_actquant(\n",
    "    model,\n",
    "    layers=[nn.Linear,\n",
    "            ActQuantWrapper,\n",
    "            RotatedHead,\n",
    "            RotatedLinearIn,\n",
    "            RotatedLinearOut,\n",
    "            RotatedOVProj]\n",
    ")\n",
    "\n",
    "qlayers = quant_utils.find_qlayers(\n",
    "    model.model,\n",
    "    [nn.Linear,\n",
    "     ActQuantWrapper,\n",
    "     RotatedHead,\n",
    "     RotatedLinearIn,\n",
    "     RotatedLinearOut,\n",
    "     RotatedOVProj])\n",
    "\n",
    "for name in qlayers:\n",
    "    if 'down_proj' in name:\n",
    "        had_K, K = hadamard_utils.get_hadK(model.config.intermediate_size)\n",
    "        qlayers[name].online_full_had = True\n",
    "        qlayers[name].had_K = had_K\n",
    "        qlayers[name].K = K\n",
    "        qlayers[name].fp32_had = args.fp32_had\n",
    "    # if 'o_proj' in name:\n",
    "    #     had_K, K = hadamard_utils.get_hadK(model.config.num_attention_heads)\n",
    "    #     qlayers[name].online_partial_had = True\n",
    "    #     qlayers[name].had_K = had_K\n",
    "    #     qlayers[name].K = K\n",
    "    #     qlayers[name].had_dim = model.config.hidden_size//model.config.num_attention_heads\n",
    "    #     qlayers[name].fp32_had = args.fp32_had\n",
    "\n",
    "if args.w_bits < 16:\n",
    "    save_dict = {}\n",
    "    if args.load_qmodel_path: # Load Quantized Rotated Model\n",
    "        assert args.rotate, \"Model should be rotated to load a quantized model!\"\n",
    "        assert not args.save_qmodel_path, \"Cannot save a quantized model if it is already loaded!\"\n",
    "        print(\"Load quantized model from \", args.load_qmodel_path)\n",
    "        save_dict = torch.load(args.load_qmodel_path)\n",
    "        model.load_state_dict(save_dict[\"model\"])\n",
    "        \n",
    "    elif not args.w_rtn: # GPTQ Weight Quantization\n",
    "        assert \"llama\" in args.model, \"Only llama is supported for GPTQ!\"\n",
    "        \n",
    "        trainloader = data_utils.get_loaders(\n",
    "            args.cal_dataset, nsamples=args.nsamples,\n",
    "            seed=args.seed, model=args.model,\n",
    "            seqlen=model.seqlen, eval_mode=False\n",
    "        )\n",
    "        quantizers = gptq_utils.gptq_fwrd(\n",
    "            model,\n",
    "            trainloader,\n",
    "            utils.DEV,\n",
    "            args,\n",
    "            [RotatedHead, RotatedLinearIn, RotatedLinearOut, RotatedOVProj])\n",
    "        save_dict[\"w_quantizers\"] = quantizers\n",
    "    else: # RTN Weight Quantization\n",
    "        quantizers = gptq_utils.rtn_fwrd(model, utils.DEV, args)\n",
    "        save_dict[\"w_quantizers\"] = quantizers\n",
    "        \n",
    "    if args.save_qmodel_path:\n",
    "        save_dict[\"model\"] = model.state_dict()\n",
    "        torch.save(save_dict, args.save_qmodel_path)\n",
    "\n",
    "if args.a_bits < 16 or args.v_bits < 16:\n",
    "    qlayers = quant_utils.find_qlayers(model, layers=[quant_utils.ActQuantWrapper])\n",
    "    down_proj_groupsize = -1\n",
    "    if args.a_groupsize > 0 and \"llama\" in args.model:\n",
    "        down_proj_groupsize = utils.llama_down_proj_groupsize(model, args.a_groupsize)\n",
    "    \n",
    "    for name in qlayers:            \n",
    "        layer_input_bits = args.a_bits\n",
    "        layer_groupsize = args.a_groupsize\n",
    "        layer_a_sym = not(args.a_asym)\n",
    "        layer_a_clip = args.a_clip_ratio\n",
    "        \n",
    "        if 'v_proj' in name and args.v_bits < 16: #Set the v_proj precision\n",
    "            qlayers[name].out_quantizer.configure(bits=args.v_bits,\n",
    "                                            groupsize=args.v_groupsize,\n",
    "                                            sym=not(args.v_asym),\n",
    "                                            clip_ratio=args.v_clip_ratio)\n",
    "        \n",
    "        if 'lm_head' in name: #Skip lm_head quantization   \n",
    "            layer_input_bits = 16\n",
    "        \n",
    "        if 'down_proj' in name: #Set the down_proj precision\n",
    "            if args.int8_down_proj:\n",
    "                layer_input_bits = 8\n",
    "            layer_groupsize = down_proj_groupsize\n",
    "\n",
    "            \n",
    "        qlayers[name].quantizer.configure(bits=layer_input_bits,\n",
    "                                            groupsize=layer_groupsize,\n",
    "                                            sym=layer_a_sym,\n",
    "                                            clip_ratio=layer_a_clip)\n",
    "\n",
    "if args.k_bits < 16:\n",
    "    if args.k_pre_rope:\n",
    "        raise NotImplementedError(\"Pre-RoPE quantization is not supported yet!\")\n",
    "    else:\n",
    "        rope_function_name = model_utils.get_rope_function_name(model)\n",
    "        layers = model_utils.get_layers(model)\n",
    "        k_quant_config = {'k_bits':args.k_bits, \"k_groupsize\": args.k_groupsize,\n",
    "                                        \"k_sym\": not(args.k_asym), \"k_clip_ratio\": args.k_clip_ratio}\n",
    "        for layer in layers:\n",
    "            rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(\n",
    "                        layer.self_attn, \n",
    "                        rope_function_name, \n",
    "                        config=model.config,\n",
    "                        **k_quant_config)\n",
    "# rope_function_name = model_utils.get_rope_function_name(model)\n",
    "# layers = model_utils.get_layers(model)\n",
    "# k_quant_config = {'k_bits': 16, \"k_groupsize\": args.k_groupsize,\n",
    "#                     \"k_sym\": not(args.k_asym), \"k_clip_ratio\": args.k_clip_ratio}\n",
    "\n",
    "# for idx, layer in enumerate(layers):\n",
    "#     print('Wrapping QK', idx)\n",
    "#     rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(\n",
    "#                 layer.self_attn, \n",
    "#                 rope_function_name, \n",
    "#                 config=model.config,\n",
    "#                 **k_quant_config)\n",
    "\n",
    "#model = model.to(utils.DEV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 50758680576, 1: 50758680576, 2: 50758680576, 3: 50758680576, 4: 50758680576, 5: 50758680576, 6: 50758680576, 7: 50758680576, 'cpu': 245580857344}\n",
      "{0: 2219574570, 1: 2219574570, 2: 2219574570, 3: 2219574570, 4: 2219574570, 5: 2219574570, 6: 2219574570, 7: 50758680576, 'cpu': 245580857344}\n",
      "\n",
      "Treating module model.\n",
      "Not enough space on 0 to put model (space available 2219574554, module size 13423478532).\n",
      "Splitting model.\n",
      "\n",
      "Treating module model.embed_tokens.\n",
      "Putting model.embed_tokens (size=396361728) on 0 (available=1812477170).\n",
      "\n",
      "Treating module model.layers.\n",
      "Not enough space on 0 to put model.layers (space available 1823212826, module size 13027116800).\n",
      "Splitting model.layers.\n",
      "\n",
      "Treating module model.layers.0.\n",
      "Putting model.layers.0 (size=407097400) on 0 (available=1416115442).\n",
      "\n",
      "Treating module model.layers.1.\n",
      "Putting model.layers.1 (size=407097400) on 0 (available=1009018042).\n",
      "\n",
      "Treating module model.layers.2.\n",
      "Putting model.layers.2 (size=407097400) on 0 (available=601920642).\n",
      "\n",
      "Treating module model.layers.3.\n",
      "Not enough space on 0 to put model.layers.3 (space available 194823242, module size 407097400).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module model.layers.3.\n",
      "Putting model.layers.3 (size=407097400) on 1 (available=2219574570).\n",
      "\n",
      "Treating module model.layers.4.\n",
      "Putting model.layers.4 (size=407097400) on 1 (available=1812477170).\n",
      "\n",
      "Treating module model.layers.5.\n",
      "Putting model.layers.5 (size=407097400) on 1 (available=1405379770).\n",
      "\n",
      "Treating module model.layers.6.\n",
      "Putting model.layers.6 (size=407097400) on 1 (available=998282370).\n",
      "\n",
      "Treating module model.layers.7.\n",
      "Putting model.layers.7 (size=407097400) on 1 (available=591184970).\n",
      "\n",
      "Treating module model.layers.8.\n",
      "Not enough space on 1 to put model.layers.8 (space available 184087570, module size 407097400).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module model.layers.8.\n",
      "Putting model.layers.8 (size=407097400) on 2 (available=2219574570).\n",
      "\n",
      "Treating module model.layers.9.\n",
      "Putting model.layers.9 (size=407097400) on 2 (available=1812477170).\n",
      "\n",
      "Treating module model.layers.10.\n",
      "Putting model.layers.10 (size=407097400) on 2 (available=1405379770).\n",
      "\n",
      "Treating module model.layers.11.\n",
      "Putting model.layers.11 (size=407097400) on 2 (available=998282370).\n",
      "\n",
      "Treating module model.layers.12.\n",
      "Putting model.layers.12 (size=407097400) on 2 (available=591184970).\n",
      "\n",
      "Treating module model.layers.13.\n",
      "Not enough space on 2 to put model.layers.13 (space available 184087570, module size 407097400).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module model.layers.13.\n",
      "Putting model.layers.13 (size=407097400) on 3 (available=2219574570).\n",
      "\n",
      "Treating module model.layers.14.\n",
      "Putting model.layers.14 (size=407097400) on 3 (available=1812477170).\n",
      "\n",
      "Treating module model.layers.15.\n",
      "Putting model.layers.15 (size=407097400) on 3 (available=1405379770).\n",
      "\n",
      "Treating module model.layers.16.\n",
      "Putting model.layers.16 (size=407097400) on 3 (available=998282370).\n",
      "\n",
      "Treating module model.layers.17.\n",
      "Putting model.layers.17 (size=407097400) on 3 (available=591184970).\n",
      "\n",
      "Treating module model.layers.18.\n",
      "Not enough space on 3 to put model.layers.18 (space available 184087570, module size 407097400).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module model.layers.18.\n",
      "Putting model.layers.18 (size=407097400) on 4 (available=2219574570).\n",
      "\n",
      "Treating module model.layers.19.\n",
      "Putting model.layers.19 (size=407097400) on 4 (available=1812477170).\n",
      "\n",
      "Treating module model.layers.20.\n",
      "Putting model.layers.20 (size=407097400) on 4 (available=1405379770).\n",
      "\n",
      "Treating module model.layers.21.\n",
      "Putting model.layers.21 (size=407097400) on 4 (available=998282370).\n",
      "\n",
      "Treating module model.layers.22.\n",
      "Putting model.layers.22 (size=407097400) on 4 (available=591184970).\n",
      "\n",
      "Treating module model.layers.23.\n",
      "Not enough space on 4 to put model.layers.23 (space available 184087570, module size 407097400).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module model.layers.23.\n",
      "Putting model.layers.23 (size=407097400) on 5 (available=2219574570).\n",
      "\n",
      "Treating module model.layers.24.\n",
      "Putting model.layers.24 (size=407097400) on 5 (available=1812477170).\n",
      "\n",
      "Treating module model.layers.25.\n",
      "Putting model.layers.25 (size=407097400) on 5 (available=1405379770).\n",
      "\n",
      "Treating module model.layers.26.\n",
      "Putting model.layers.26 (size=407097400) on 5 (available=998282370).\n",
      "\n",
      "Treating module model.layers.27.\n",
      "Putting model.layers.27 (size=407097400) on 5 (available=591184970).\n",
      "\n",
      "Treating module model.layers.28.\n",
      "Not enough space on 5 to put model.layers.28 (space available 184087570, module size 407097400).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module model.layers.28.\n",
      "Putting model.layers.28 (size=407097400) on 6 (available=2219574570).\n",
      "\n",
      "Treating module model.layers.29.\n",
      "Putting model.layers.29 (size=407097400) on 6 (available=1812477170).\n",
      "\n",
      "Treating module model.layers.30.\n",
      "Putting model.layers.30 (size=407097400) on 6 (available=1405379770).\n",
      "\n",
      "Treating module model.layers.31.\n",
      "Putting model.layers.31 (size=407097400) on 6 (available=998282370).\n",
      "\n",
      "Treating module model.norm.\n",
      "Putting model.norm (size=4) on 6 (available=591184970).\n",
      "\n",
      "Treating module lm_head.\n",
      "Putting lm_head (size=262144032) on 6 (available=591184966).\n",
      "OrderedDict([('model.embed_tokens', 0), ('model.layers.0', 0), ('model.layers.1', 0), ('model.layers.2', 0), ('model.layers.3', 1), ('model.layers.4', 1), ('model.layers.5', 1), ('model.layers.6', 1), ('model.layers.7', 1), ('model.layers.8', 2), ('model.layers.9', 2), ('model.layers.10', 2), ('model.layers.11', 2), ('model.layers.12', 2), ('model.layers.13', 3), ('model.layers.14', 3), ('model.layers.15', 3), ('model.layers.16', 3), ('model.layers.17', 3), ('model.layers.18', 4), ('model.layers.19', 4), ('model.layers.20', 4), ('model.layers.21', 4), ('model.layers.22', 4), ('model.layers.23', 5), ('model.layers.24', 5), ('model.layers.25', 5), ('model.layers.26', 5), ('model.layers.27', 5), ('model.layers.28', 6), ('model.layers.29', 6), ('model.layers.30', 6), ('model.layers.31', 6), ('model.norm', 6), ('lm_head', 6)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU memory (from distribute_model): 40.88 -> 40.88 GB (0.00 GB)\n"
     ]
    }
   ],
   "source": [
    "utils.distribute_model(model)\n",
    "\n",
    "#for p in model.parameters():\n",
    "#    p.requires_grad = False\n",
    "\n",
    "Q1.requires_grad=True\n",
    "for Q2 in Q2s:\n",
    "    Q2.requires_grad=True\n",
    "\n",
    "#optimizer = torch.optim.Adam([Q1, Q2], lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stiefel import stiefel_optimizer\n",
    "optimizer = stiefel_optimizer.SGDG([\n",
    "    {'params': [Q1] + Q2s, 'lr': 1.5, 'momentum': 0.0, 'stiefel': True}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data_utils.get_loaders(\n",
    "    args.cal_dataset, nsamples=args.nsamples,\n",
    "    seed=args.seed, model=args.model,\n",
    "    seqlen=model.seqlen, eval_mode=False\n",
    ")\n",
    "\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "label_smoother = LabelSmoother(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, optimizer, trainloader = accelerator.prepare(model, optimizer, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm.tqdm(range(0 + 1, 100 + 1), desc=\"Training progress\",\n",
    "            total=100, dynamic_ncols=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_stack = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 = torch.zeros_like(Q1).requires_grad_(False)\n",
    "# M2s = []\n",
    "\n",
    "# for Q2 in Q2s:\n",
    "#     M2 = torch.zeros_like(Q2).requires_grad_(False)\n",
    "#     M2s.append(M2)\n",
    "\n",
    "# beta = 0.9\n",
    "# epsilon = 1e-8\n",
    "# s = 5\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def cayley_sgd(X, M, l, beta, epsilon, q, s):\n",
    "#     if X.grad is not None:\n",
    "#         M = beta * M - X.grad\n",
    "#         #print('M', M.isnan().any().item())\n",
    "#         MK = torch.matmul(M, X.T)\n",
    "#         W_hat = MK - 0.5 * torch.matmul(X, torch.matmul(X.T, MK))\n",
    "#         #print('W_hat', W_hat.isnan().any().item())\n",
    "#         W = W_hat - W_hat.T\n",
    "        \n",
    "#         M = torch.matmul(W, X)\n",
    "        \n",
    "#         alpha = min(l, 2. * q / (torch.norm(W) + epsilon))\n",
    "#         #print('alpha', alpha)\n",
    "#         Y = X + alpha * M\n",
    "        \n",
    "#         for i in range(s):\n",
    "#             Y = X + alpha / 2 * torch.matmul(W, X + Y)\n",
    "        \n",
    "#         X.data = Y\n",
    "#         X.grad.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(iter, total_iter, max_lr, min_lr):\n",
    "    return max_lr - iter / total_iter * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   1%|          | 1/100 [03:25<5:38:29, 205.15s/it, CE=12.595, ortho1=0.000, ortho2=0.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for iteration in pbar:\n",
    "    if not idx_stack:\n",
    "        idx_stack = list(range(0, len(trainloader)))\n",
    "    \n",
    "    idx = idx_stack.pop(randint(0, len(idx_stack) - 1))\n",
    "    \n",
    "    data = trainloader[idx]\n",
    "    \n",
    "    input = data[0]#.to(utils.DEV)\n",
    "    target = data[1]#.to(utils.DEV)\n",
    "    #print()\n",
    "    output = model(input)\n",
    "    \n",
    "    loss = label_smoother(output, input, shift_labels=True)\n",
    "    \n",
    "    Q1.retain_grad()\n",
    "    for Q2 in Q2s:\n",
    "        Q2.retain_grad()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #accelerator.backward(loss)\n",
    "    lr = lr_schedule(iteration, 100, 1.5, 0)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    for p in [Q1]+Q2s:\n",
    "        print(p.grad is None)\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar.set_postfix(\n",
    "            {'CE': f'{loss.item():.3f}',\n",
    "             'ortho1': f'{(torch.matmul(Q1, Q1.T) - torch.eye(Q1.size(0)).to(Q1.device)).sum().item():.3f}',\n",
    "             'ortho2': f'{(torch.matmul(Q2, Q2.T) - torch.eye(Q2.size(0)).to(Q2.device)).sum().item():.3f}',\n",
    "             #'det(Q)': f'{torch.linalg.det(Q):.3f}'\n",
    "             }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "for layer in model.model.layers:\n",
    "    print(layer.self_attn.v_proj.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0154, -0.0159, -0.0153,  ..., -0.0157, -0.0155, -0.0157],\n",
       "        [ 0.0160, -0.0157,  0.0155,  ..., -0.0158,  0.0159, -0.0156],\n",
       "        [ 0.0156,  0.0157, -0.0153,  ...,  0.0157, -0.0157, -0.0158],\n",
       "        ...,\n",
       "        [-0.0156,  0.0157, -0.0158,  ...,  0.0157, -0.0155,  0.0157],\n",
       "        [-0.0157, -0.0155,  0.0158,  ..., -0.0163,  0.0156,  0.0155],\n",
       "        [-0.0155,  0.0157,  0.0156,  ...,  0.0156,  0.0159, -0.0160]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0884, -0.0884, -0.0884,  ..., -0.0884, -0.0884, -0.0884],\n",
       "        [ 0.0884, -0.0884,  0.0884,  ..., -0.0884,  0.0884, -0.0884],\n",
       "        [ 0.0884,  0.0884, -0.0884,  ...,  0.0884, -0.0884, -0.0884],\n",
       "        ...,\n",
       "        [-0.0884,  0.0884, -0.0884,  ..., -0.0884,  0.0884, -0.0884],\n",
       "        [ 0.0884,  0.0884, -0.0884,  ..., -0.0884,  0.0884,  0.0884],\n",
       "        [-0.0884,  0.0884,  0.0884,  ..., -0.0884, -0.0884,  0.0884]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Q1, 'Q1.pt')\n",
    "torch.save(Q2, 'Q2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mhead_dim\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.layers[0].self_attn.head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
