{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "from accelerate.utils import get_balanced_memory\n",
    "\n",
    "from quant_utils import ActQuantizer\n",
    "\n",
    "from utils import distribute_model\n",
    "import hadamard_utils\n",
    "import fast_hadamard_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatedLinear(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        if Q is None:\n",
    "            W_ = W\n",
    "            b_ = b\n",
    "\n",
    "        else:\n",
    "        \n",
    "        #if W.device != self.Q.device:\n",
    "        #    self.Q = self.Q.to(W.device)\n",
    "        \n",
    "            W_ = torch.matmul(Q.to(W.device).T, W.to(dtype=Q.dtype)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('linear out')\n",
    "            #print(W_.grad_fn)\n",
    "            if b is not None:\n",
    "                b_ = torch.matmul(Q.to(W.device).T, b.to(dtype=Q.dtype)).to(dtype=b.dtype)\n",
    "            else:\n",
    "                b_ = b\n",
    "        \n",
    "        x = torch.nn.functional.linear(x, W_, b_)\n",
    "        return x\n",
    "        # return F.linear(\n",
    "        #     x, W_, b_\n",
    "        # )\n",
    "\n",
    "class RotatedOVProj(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        output=False,\n",
    "        nheads=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        \n",
    "        self.output = output\n",
    "        self.nheads = nheads\n",
    "    \n",
    "    def forward(self, x, Qin=None, Qout=None):\n",
    "        W = self.weight\n",
    "        \n",
    "        # if W.device != self.Qin.device:\n",
    "        #     self.Qin = self.Qin.to(W.device)\n",
    "        \n",
    "        # if W.device != self.Qout.device:\n",
    "        #     self.Qout = self.Qout.to(W.device)\n",
    "        \n",
    "        if Qin is not None:\n",
    "            if self.output:\n",
    "                W_ = torch.matmul(W.to(dtype=Qin.dtype), Qin.to(W.device)).to(dtype=W.dtype)\n",
    "            else:\n",
    "                W_ = W.to(dtype=Qin.dtype).reshape(W.size(0), self.nheads, -1)\n",
    "                print('linear o', W_.size(), Qin.size())\n",
    "                W_ = torch.einsum('inh,hj->inj', W_, Qin.to(W.device)).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear o', W.grad_fn, W_.grad_fn, self.Qin.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        if Qout is not None:\n",
    "            if self.output:\n",
    "                W_ = W_.to(dtype=Qout.dtype).reshape(self.nheads, -1, W.size(1))\n",
    "                print('linear v', W_.size(), Qin.size())\n",
    "                W_ = torch.einsum('ih,nhj->nij', Qout.to(W.device).T, W_).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear v', W.grad_fn, W_.grad_fn, self.Qout.grad_fn)\n",
    "                #print(W_.grad_fn)\n",
    "            else:\n",
    "                W_ = torch.matmul(Qout.to(W.device).T, W_.to(dtype=Qout.dtype)).to(dtype=W.dtype)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = torch.nn.functional.linear(x, W_)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActQuantWrapper(torch.nn.Module):\n",
    "    '''\n",
    "        This class is a wrapper for the activation quantization.\n",
    "        We extract the FP features in the forward pass and quantize the rest using\n",
    "        the self.quantizer object.\n",
    "        If a rotation Q is provided, the weight matrix will be rotated,\n",
    "        a pre-forward hook will be registerd to rotate the activation before quantization.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, module:torch.nn.Linear):\n",
    "        super(ActQuantWrapper, self).__init__()\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        self.module = module\n",
    "        self.weight = module.weight\n",
    "        self.bias = module.bias\n",
    "        self.quantizer = ActQuantizer()\n",
    "        self.out_quantizer = ActQuantizer()\n",
    "        self.register_buffer('had_K', torch.tensor(0))\n",
    "        self._buffers['had_K'] = None\n",
    "        self.K = 1\n",
    "        self.online_full_had = False\n",
    "        self.online_partial_had = False\n",
    "        self.had_dim = 0\n",
    "        self.fp32_had = False\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        str_ = f'Input Quantizer Bits: {self.quantizer.bits}'\n",
    "        if self.quantizer.bits < 16:\n",
    "            str_ += f' (Asymmetric Per-Token)' if not self.quantizer.sym else f' (Symmetric Per-Token)'\n",
    "\n",
    "        str_ += f'\\nOutput Quantizer Bits: {self.out_quantizer.bits}'\n",
    "        if self.out_quantizer.bits < 16:\n",
    "            str_ += f' (Asymmetric Per-Token)' if not self.out_quantizer.sym else f' (Symmetric Per-Token)'\n",
    "\n",
    "        return str_\n",
    "\n",
    "    def forward(self, x, Q=None, Qin=None, Qout=None):\n",
    "        x_dtype = x.dtype\n",
    "\n",
    "        # Rotate, if needed\n",
    "        if self.online_full_had:\n",
    "            \n",
    "            if self.fp32_had: # Full Hadamard in FP32\n",
    "                x = hadamard_utils.matmul_hadU_cuda(x.float(), self.had_K, self.K).to(x_dtype)\n",
    "            else: # Full Hadamard in FP16\n",
    "                x = hadamard_utils.matmul_hadU_cuda(x, self.had_K, self.K)\n",
    "            \n",
    "        elif self.online_partial_had:\n",
    "            # todo: implement this in QAttention to avoid reshaping!\n",
    "            \n",
    "            if self.fp32_had:\n",
    "                x = x.float()\n",
    "                \n",
    "            init_shape = x.shape\n",
    "            if self.K == 1:\n",
    "                x = fast_hadamard_transform.hadamard_transform(x.reshape(-1, init_shape[-1]//self.had_dim, self.had_dim).transpose(1, 2),\n",
    "                                                               scale=1/math.sqrt(init_shape[-1]//self.had_dim)).transpose(1, 2)\n",
    "            else:\n",
    "                x = (self.had_K.to(x.dtype) @ x.reshape(-1, init_shape[-1]//self.had_dim, self.had_dim)) / math.sqrt(init_shape[-1]//self.had_dim)\n",
    "                \n",
    "            if self.fp32_had:\n",
    "                x = x.to(x_dtype)\n",
    "            x = x.reshape(init_shape)\n",
    "\n",
    "        if self.quantizer.bits < 16: #Quantize, if needed\n",
    "            self.quantizer.find_params(x)\n",
    "            x = self.quantizer(x).to(x_dtype)\n",
    "            self.quantizer.free()\n",
    "\n",
    "        if isinstance(self.module, RotatedLinear):\n",
    "            x = self.module(x, Q=Q).to(x_dtype)\n",
    "        elif isinstance(self.module, RotatedOVProj):\n",
    "            x = self.module(x, Qin=Qin, Qout=Qout).to(x_dtype)\n",
    "        else:\n",
    "            x = self.module(x).to(x_dtype)\n",
    "\n",
    "        if self.out_quantizer.bits < 16: #Quantize the output, if needed\n",
    "            self.out_quantizer.find_params(x)\n",
    "            x = self.out_quantizer(x).to(x_dtype)\n",
    "            self.out_quantizer.free()\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = RotatedLinear(4, 4, False)\n",
    "b = RotatedLinear(4, 4, False)\n",
    "c = RotatedOVProj(4, 4, False, output=True, nheads=2)\n",
    "d = RotatedOVProj(4, 4, False, output=False, nheads=2)\n",
    "e = RotatedOVProj(4, 4, False, output=True, nheads=2)\n",
    "f = RotatedOVProj(4, 4, False, output=False, nheads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVWrapper(nn.Module):\n",
    "    def __init__(self, O, V):\n",
    "        super().__init__()\n",
    "        self.O = O\n",
    "        self.V = V\n",
    "    \n",
    "    def forward(self, x, Q1, Q2):\n",
    "        x = self.V(x, Qin=Q1, Qout=Q2)\n",
    "        x = self.O(x, Qin=Q2, Qout=Q1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ActQuantWrapper(a)\n",
    "B = ActQuantWrapper(b)\n",
    "C = ActQuantWrapper(c)\n",
    "D = ActQuantWrapper(d)\n",
    "E = ActQuantWrapper(e)\n",
    "F = ActQuantWrapper(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        x = self.module(x, Q=Q)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ = Wrapper(A)\n",
    "B_ = Wrapper(B)\n",
    "# C_ = Wrapper(C)\n",
    "# D_ = Wrapper(D)\n",
    "# E_ = Wrapper(E)\n",
    "# F_ = Wrapper(F)\n",
    "L1 = OVWrapper(D, C)\n",
    "L2 = OVWrapper(F, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopModel(nn.Module):\n",
    "    def __init__(self, heads, modules):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(heads)\n",
    "        self.layers = nn.ModuleList(modules)\n",
    "    \n",
    "    def forward(self, x, Q1=None, Q2s=None):\n",
    "        for layer in self.heads:\n",
    "            x = layer(x, Q=Q1)\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if Q2s is not None:\n",
    "                Q2 = Q2s[idx]\n",
    "            x = layer(x, Q1=Q1, Q2=Q2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopModel([A_, B_], [L1, L2])\n",
    "q = torch.ones((4, 4)) / 4\n",
    "Q1 = nn.Parameter(q, requires_grad=True)\n",
    "\n",
    "Q2s = torch.ones((2, 2, 2)) / 2\n",
    "Q2s = nn.Parameter(Q2s, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopModel(\n",
       "  (heads): ModuleList(\n",
       "    (0-1): 2 x Wrapper(\n",
       "      (module): ActQuantWrapper(\n",
       "        Input Quantizer Bits: 16\n",
       "        Output Quantizer Bits: 16\n",
       "        (module): RotatedLinear(in_features=4, out_features=4, bias=False)\n",
       "        (quantizer): ActQuantizer()\n",
       "        (out_quantizer): ActQuantizer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x OVWrapper(\n",
       "      (O): ActQuantWrapper(\n",
       "        Input Quantizer Bits: 16\n",
       "        Output Quantizer Bits: 16\n",
       "        (module): RotatedOVProj(in_features=4, out_features=4, bias=False)\n",
       "        (quantizer): ActQuantizer()\n",
       "        (out_quantizer): ActQuantizer()\n",
       "      )\n",
       "      (V): ActQuantWrapper(\n",
       "        Input Quantizer Bits: 16\n",
       "        Output Quantizer Bits: 16\n",
       "        (module): RotatedOVProj(in_features=4, out_features=4, bias=False)\n",
       "        (quantizer): ActQuantizer()\n",
       "        (out_quantizer): ActQuantizer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 50758680576, 1: 50758680576, 2: 50758680576, 3: 50758680576, 4: 50758680576, 5: 50758680576, 6: 50758680576, 7: 50758680576, 'cpu': 255111868416}\n",
      "{0: 92, 1: 92, 2: 92, 3: 92, 4: 92, 5: 92, 6: 92, 7: 50758680576, 'cpu': 255111868416}\n",
      "\n",
      "Treating module heads.\n",
      "Not enough space on 0 to put heads (space available 76, module size 192).\n",
      "Splitting heads.\n",
      "\n",
      "Treating module heads.0.\n",
      "Not enough space on 0 to put heads.0 (space available 76, module size 96).\n",
      "Splitting heads.0.\n",
      "\n",
      "Treating module heads.0.module.\n",
      "Not enough space on 0 to put heads.0.module (space available 76, module size 96).\n",
      "Splitting heads.0.module.\n",
      "\n",
      "Treating module heads.0.module.weight.\n",
      "  Found the relevant tied param groups [['heads.0.module.module.weight', 'heads.0.module.weight']]\n",
      "  So those parameters need to be taken into account ['heads.0.module.module.weight']\n",
      "Modules to treat ['heads.0.module.module', 'heads.0.module.quantizer', 'heads.0.module.out_quantizer', 'heads.1', 'layers']\n",
      "Tied params ['heads.0.module.module.weight']\n",
      "  It looks like heads.0.module.weight is going to fit on 0 but we have tied parameters to account for.\n",
      "  - Names ['heads.0.module.module.weight']\n",
      "  - Module names ['heads.0.module.module']\n",
      "Putting heads.0.module.weight and ['heads.0.module.module'] on 0.\n",
      "\n",
      "Treating module heads.0.module.quantizer.\n",
      "Not enough space on 0 to put heads.0.module.quantizer (space available 12, module size 16).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module heads.0.module.quantizer.\n",
      "Putting heads.0.module.quantizer (size=16) on 1 (available=92).\n",
      "\n",
      "Treating module heads.0.module.out_quantizer.\n",
      "Putting heads.0.module.out_quantizer (size=16) on 1 (available=76).\n",
      "\n",
      "Treating module heads.1.\n",
      "Not enough space on 1 to put heads.1 (space available 60, module size 96).\n",
      "Splitting heads.1.\n",
      "\n",
      "Treating module heads.1.module.\n",
      "Not enough space on 1 to put heads.1.module (space available 60, module size 96).\n",
      "Splitting heads.1.module.\n",
      "\n",
      "Treating module heads.1.module.weight.\n",
      "  Found the relevant tied param groups [['heads.1.module.module.weight', 'heads.1.module.weight']]\n",
      "  So those parameters need to be taken into account ['heads.1.module.module.weight']\n",
      "Not enough space on 1 to put heads.1.module.weight (space available 60, module size 64).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module heads.1.module.weight.\n",
      "  Found the relevant tied param groups [['heads.1.module.module.weight', 'heads.1.module.weight']]\n",
      "  So those parameters need to be taken into account ['heads.1.module.module.weight']\n",
      "Modules to treat ['heads.1.module.module', 'heads.1.module.quantizer', 'heads.1.module.out_quantizer', 'layers']\n",
      "Tied params ['heads.1.module.module.weight']\n",
      "  It looks like heads.1.module.weight is going to fit on 2 but we have tied parameters to account for.\n",
      "  - Names ['heads.1.module.module.weight']\n",
      "  - Module names ['heads.1.module.module']\n",
      "Putting heads.1.module.weight and ['heads.1.module.module'] on 2.\n",
      "\n",
      "Treating module heads.1.module.quantizer.\n",
      "Putting heads.1.module.quantizer (size=16) on 2 (available=28).\n",
      "\n",
      "Treating module heads.1.module.out_quantizer.\n",
      "Not enough space on 2 to put heads.1.module.out_quantizer (space available 12, module size 16).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module heads.1.module.out_quantizer.\n",
      "Putting heads.1.module.out_quantizer (size=16) on 3 (available=92).\n",
      "\n",
      "Treating module layers.\n",
      "Not enough space on 3 to put layers (space available 76, module size 384).\n",
      "Splitting layers.\n",
      "\n",
      "Treating module layers.0.\n",
      "Not enough space on 3 to put layers.0 (space available 76, module size 192).\n",
      "Splitting layers.0.\n",
      "\n",
      "Treating module layers.0.O.\n",
      "Not enough space on 3 to put layers.0.O (space available 76, module size 96).\n",
      "Splitting layers.0.O.\n",
      "\n",
      "Treating module layers.0.O.weight.\n",
      "  Found the relevant tied param groups [['layers.0.O.module.weight', 'layers.0.O.weight']]\n",
      "  So those parameters need to be taken into account ['layers.0.O.module.weight']\n",
      "Modules to treat ['layers.0.O.module', 'layers.0.O.quantizer', 'layers.0.O.out_quantizer', 'layers.0.V', 'layers.1']\n",
      "Tied params ['layers.0.O.module.weight']\n",
      "  It looks like layers.0.O.weight is going to fit on 3 but we have tied parameters to account for.\n",
      "  - Names ['layers.0.O.module.weight']\n",
      "  - Module names ['layers.0.O.module']\n",
      "Putting layers.0.O.weight and ['layers.0.O.module'] on 3.\n",
      "\n",
      "Treating module layers.0.O.quantizer.\n",
      "Not enough space on 3 to put layers.0.O.quantizer (space available 12, module size 16).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.O.quantizer.\n",
      "Putting layers.0.O.quantizer (size=16) on 4 (available=92).\n",
      "\n",
      "Treating module layers.0.O.out_quantizer.\n",
      "Putting layers.0.O.out_quantizer (size=16) on 4 (available=76).\n",
      "\n",
      "Treating module layers.0.V.\n",
      "Not enough space on 4 to put layers.0.V (space available 60, module size 96).\n",
      "Splitting layers.0.V.\n",
      "\n",
      "Treating module layers.0.V.weight.\n",
      "  Found the relevant tied param groups [['layers.0.V.module.weight', 'layers.0.V.weight']]\n",
      "  So those parameters need to be taken into account ['layers.0.V.module.weight']\n",
      "Not enough space on 4 to put layers.0.V.weight (space available 60, module size 64).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.V.weight.\n",
      "  Found the relevant tied param groups [['layers.0.V.module.weight', 'layers.0.V.weight']]\n",
      "  So those parameters need to be taken into account ['layers.0.V.module.weight']\n",
      "Modules to treat ['layers.0.V.module', 'layers.0.V.quantizer', 'layers.0.V.out_quantizer', 'layers.1']\n",
      "Tied params ['layers.0.V.module.weight']\n",
      "  It looks like layers.0.V.weight is going to fit on 5 but we have tied parameters to account for.\n",
      "  - Names ['layers.0.V.module.weight']\n",
      "  - Module names ['layers.0.V.module']\n",
      "Putting layers.0.V.weight and ['layers.0.V.module'] on 5.\n",
      "\n",
      "Treating module layers.0.V.quantizer.\n",
      "Putting layers.0.V.quantizer (size=16) on 5 (available=28).\n",
      "\n",
      "Treating module layers.0.V.out_quantizer.\n",
      "Not enough space on 5 to put layers.0.V.out_quantizer (space available 12, module size 16).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.V.out_quantizer.\n",
      "Putting layers.0.V.out_quantizer (size=16) on 6 (available=92).\n",
      "\n",
      "Treating module layers.1.\n",
      "Not enough space on 6 to put layers.1 (space available 76, module size 192).\n",
      "Splitting layers.1.\n",
      "\n",
      "Treating module layers.1.O.\n",
      "Not enough space on 6 to put layers.1.O (space available 76, module size 96).\n",
      "Splitting layers.1.O.\n",
      "\n",
      "Treating module layers.1.O.weight.\n",
      "  Found the relevant tied param groups [['layers.1.O.module.weight', 'layers.1.O.weight']]\n",
      "  So those parameters need to be taken into account ['layers.1.O.module.weight']\n",
      "Modules to treat ['layers.1.O.module', 'layers.1.O.quantizer', 'layers.1.O.out_quantizer', 'layers.1.V']\n",
      "Tied params ['layers.1.O.module.weight']\n",
      "  It looks like layers.1.O.weight is going to fit on 6 but we have tied parameters to account for.\n",
      "  - Names ['layers.1.O.module.weight']\n",
      "  - Module names ['layers.1.O.module']\n",
      "Putting layers.1.O.weight and ['layers.1.O.module'] on 6.\n",
      "\n",
      "Treating module layers.1.O.quantizer.\n",
      "Not enough space on 6 to put layers.1.O.quantizer (space available 12, module size 16).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.1.O.quantizer.\n",
      "Putting layers.1.O.quantizer (size=16) on 7 (available=50758680576).\n",
      "\n",
      "Treating module layers.1.O.out_quantizer.\n",
      "Putting layers.1.O.out_quantizer (size=16) on 7 (available=50758680560).\n",
      "\n",
      "Treating module layers.1.V.\n",
      "Putting layers.1.V (size=96) on 7 (available=50758680544).\n",
      "OrderedDict([('heads.0.module.weight', 0), ('heads.0.module.module', 0), ('heads.0.module.quantizer', 1), ('heads.0.module.out_quantizer', 1), ('heads.1.module.weight', 2), ('heads.1.module.module', 2), ('heads.1.module.quantizer', 2), ('heads.1.module.out_quantizer', 3), ('layers.0.O.weight', 3), ('layers.0.O.module', 3), ('layers.0.O.quantizer', 4), ('layers.0.O.out_quantizer', 4), ('layers.0.V.weight', 5), ('layers.0.V.module', 5), ('layers.0.V.quantizer', 5), ('layers.0.V.out_quantizer', 6), ('layers.1.O.weight', 6), ('layers.1.O.module', 6), ('layers.1.O.quantizer', 7), ('layers.1.O.out_quantizer', 7), ('layers.1.V', 7)])\n"
     ]
    }
   ],
   "source": [
    "distribute_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear v torch.Size([2, 2, 4]) torch.Size([4, 4])\n",
      "linear o torch.Size([4, 2, 2]) torch.Size([2, 2])\n",
      "linear v torch.Size([2, 2, 4]) torch.Size([4, 4])\n",
      "linear o torch.Size([4, 2, 2]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "y = model(x, Q1, Q2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones(1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (y - target).pow(2).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([Q1, Q2s], 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0094, 0.0114, 0.0078, 0.0095],\n",
       "        [0.0219, 0.0235, 0.0207, 0.0220],\n",
       "        [0.0153, 0.0186, 0.0126, 0.0155],\n",
       "        [0.0018, 0.0018, 0.0018, 0.0018]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0044, 0.0044],\n",
       "         [0.0119, 0.0119]],\n",
       "\n",
       "        [[0.0027, 0.0027],\n",
       "         [0.0136, 0.0136]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q2s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.to('cuda:0')\n",
    "b = b.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.ones((4, 4)) / 4\n",
    "Q = nn.Parameter(q, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = a(x, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = b(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones(4, 4).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (y - target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0046,  0.0215, -0.0045, -0.0481],\n",
       "        [ 0.0032, -0.0150,  0.0032,  0.0336],\n",
       "        [ 0.0069, -0.0322,  0.0068,  0.0720],\n",
       "        [ 0.0020, -0.0091,  0.0019,  0.0205]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0647, -0.0342,  0.2066,  0.0121],\n",
       "        [ 0.0619,  0.0657,  0.0959,  0.0715],\n",
       "        [-0.0967, -0.0812,  0.0414, -0.0577],\n",
       "        [ 0.0437,  0.0336, -0.0464,  0.0182]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopModel(nn.Module):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.module(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatedLinear(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        \n",
    "        if Q is not None:\n",
    "            self.register_parameter('Q', Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        \n",
    "        #if W.device != self.Q.device:\n",
    "        #    self.Q = self.Q.to(W.device)\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(self.Q.to(W.device).T, W.to(dtype=self.Q.dtype)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('linear out')\n",
    "            #print(W_.grad_fn)\n",
    "            if b is not None:\n",
    "                b_ = torch.matmul(self.Q.to(W.device).T, b.to(dtype=self.Q.dtype)).to(dtype=b.dtype)\n",
    "            else:\n",
    "                b_ = b\n",
    "        else:\n",
    "            W_ = W\n",
    "            b_ = b\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_, b_\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatedOVProj(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Qin=None,\n",
    "        Qout=None,\n",
    "        output=False,\n",
    "        nheads=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Qin is not None:\n",
    "            self.register_buffer(\"Qin\", Qin)\n",
    "            #self.register_parameter(\"Qin\", Qin)\n",
    "        else:\n",
    "            self.Qin = None\n",
    "        \n",
    "        if Qout is not None:\n",
    "            self.register_buffer(\"Qout\", Qout)\n",
    "            #self.register_parameter(\"Qout\", Qout)\n",
    "        else:\n",
    "            self.Qout = None\n",
    "        \n",
    "        self.output = output\n",
    "        self.nheads = nheads\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W = self.weight\n",
    "        \n",
    "        # if W.device != self.Qin.device:\n",
    "        #     self.Qin = self.Qin.to(W.device)\n",
    "        \n",
    "        # if W.device != self.Qout.device:\n",
    "        #     self.Qout = self.Qout.to(W.device)\n",
    "        \n",
    "        if self.Qin is not None:\n",
    "            if self.output:\n",
    "                W_ = torch.matmul(W.to(dtype=self.Qin.dtype), self.Qin.to(W.device)).to(dtype=W.dtype)\n",
    "            else:\n",
    "                W_ = W.to(dtype=self.Qin.dtype).reshape(W.size(0), self.nheads, -1)\n",
    "                W_ = torch.einsum('inh,hj->inj', W_, self.Qin.to(W.device)).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear o', W.grad_fn, W_.grad_fn, self.Qin.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        if self.Qout is not None:\n",
    "            if self.output:\n",
    "                W_ = W_.to(dtype=self.Qout.dtype).reshape(self.nheads, -1, W.size(1))\n",
    "                W_ = torch.einsum('ih,nhj->nij', self.Qout.to(W.device).T, W_).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear v', W.grad_fn, W_.grad_fn, self.Qout.grad_fn)\n",
    "                #print(W_.grad_fn)\n",
    "            else:\n",
    "                W_ = torch.matmul(self.Qout.to(W.device).T, W_.to(dtype=self.Qout.dtype)).to(dtype=W.dtype)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.eye(4)\n",
    "Q = nn.Parameter(q, requires_grad=True)\n",
    "\n",
    "q1 = torch.eye(2)\n",
    "Q1 = nn.Parameter(q1, requires_grad=True)\n",
    "q2 = torch.eye(2)\n",
    "Q2 = nn.Parameter(q2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = RotatedLinear(4, 4, False, Q=Q)\n",
    "b = RotatedLinear(4, 4, False, Q=Q)\n",
    "A = ActQuantWrapper(a)\n",
    "B = ActQuantWrapper(b)\n",
    "c = RotatedOVProj(4, 4, False, Qin=Q, Qout=Q1, nheads=2, output=True)\n",
    "d = RotatedOVProj(4, 4, False, Qin=Q1, Qout=Q, nheads=2, output=False)\n",
    "C = ActQuantWrapper(c)\n",
    "D = ActQuantWrapper(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopModel([A, B, C, D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory = get_balanced_memory(\n",
    "    model,\n",
    "    no_split_module_classes=[RotatedLinear, RotatedOVProj],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 121,\n",
       " 1: 121,\n",
       " 2: 121,\n",
       " 3: 121,\n",
       " 4: 121,\n",
       " 5: 121,\n",
       " 6: 121,\n",
       " 7: 50482708480,\n",
       " 'cpu': 258156986368}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treating module layers.\n",
      "Not enough space on 0 to put layers (space available 121, module size 528).\n",
      "Splitting layers.\n",
      "\n",
      "Treating module layers.0.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 0 to put layers.0 (space available -23, module size 160).\n",
      "Splitting layers.0.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 0 to put layers.0.module (space available -23, module size 128).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 1 to put layers.0.module (space available 121, module size 128).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 2 to put layers.0.module (space available 121, module size 128).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 3 to put layers.0.module (space available 121, module size 128).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 4 to put layers.0.module (space available 121, module size 128).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 5 to put layers.0.module (space available 121, module size 128).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "Not enough space on 6 to put layers.0.module (space available 121, module size 128).\n",
      "This module cannot be split, going to the next device.\n",
      "\n",
      "Treating module layers.0.module.\n",
      "  Found the relevant tied param groups [['layers.0.module.Q', 'layers.1.module.Q']]\n",
      "  So those parameters need to be taken into account ['layers.1.module.Q']\n",
      "  It looks like layers.0.module is going to fit on 7 but we have tied parameters to account for.\n",
      "  - Names ['layers.1.module.Q']\n",
      "  - Module names ['layers.1']\n",
      "Putting layers.0.module and ['layers.1'] on 7.\n",
      "\n",
      "Treating module layers.0.quantizer.\n",
      "Putting layers.0.quantizer (size=16) on 7 (available=50482708256).\n",
      "\n",
      "Treating module layers.0.out_quantizer.\n",
      "Putting layers.0.out_quantizer (size=16) on 7 (available=50482708240).\n",
      "\n",
      "Treating module layers.2.\n",
      "Putting layers.2 (size=176) on 7 (available=50482708224).\n",
      "\n",
      "Treating module layers.3.\n",
      "Putting layers.3 (size=96) on 7 (available=50482708048).\n"
     ]
    }
   ],
   "source": [
    "device_map = infer_auto_device_map(\n",
    "    model, max_memory=max_memory, no_split_module_classes=[RotatedLinear, RotatedOVProj], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 0.],\n",
       "        [0., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "Q.requires_grad_(True)\n",
    "Q1.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 2, 4)\n",
    "y = torch.randn(1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([Q, Q1], 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)\n",
    "loss = (y - pred).pow(2).mean()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0001e+00,  3.7043e-05,  6.6987e-05,  9.8963e-05],\n",
       "        [ 1.1188e-04,  1.0001e+00, -6.4361e-05, -9.0941e-05],\n",
       "        [ 8.1210e-06, -7.2100e-05,  9.9999e-01,  1.3119e-05],\n",
       "        [-3.2840e-05, -1.2416e-04,  2.6047e-05,  1.0001e+00]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 9.9994e-01, -4.9638e-05],\n",
       "        [-4.9638e-05,  1.0002e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "pred = model(x)\n",
    "loss = (y - pred).pow(2).mean()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0003e+00,  7.4198e-05,  1.3396e-04,  1.9791e-04],\n",
       "        [ 2.2387e-04,  1.0002e+00, -1.2878e-04, -1.8199e-04],\n",
       "        [ 1.6252e-05, -1.4422e-04,  9.9997e-01,  2.6272e-05],\n",
       "        [-6.5733e-05, -2.4846e-04,  5.2157e-05,  1.0001e+00]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 9.9987e-01, -9.9302e-05],\n",
       "        [-9.9302e-05,  1.0004e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
