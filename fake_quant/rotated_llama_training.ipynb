{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from rotation_utils import random_orthogonal_matrix\n",
    "from hadamard_utils import random_hadamard_matrix, apply_exact_had_to_linear\n",
    "from quant_utils import ActQuantWrapper\n",
    "\n",
    "import utils\n",
    "import model_utils\n",
    "import data_utils\n",
    "import transformers\n",
    "import quant_utils\n",
    "import rotation_utils\n",
    "import gptq_utils\n",
    "import eval_utils\n",
    "import hadamard_utils\n",
    "import rotated_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "{'a_asym': False,\n",
      " 'a_bits': 4,\n",
      " 'a_clip_ratio': 1.0,\n",
      " 'a_groupsize': -1,\n",
      " 'act_order': False,\n",
      " 'bsz': 1,\n",
      " 'cal_dataset': 'wikitext2',\n",
      " 'capture_layer_io': False,\n",
      " 'distribute': False,\n",
      " 'eval_dataset': 'wikitext2',\n",
      " 'fp32_had': False,\n",
      " 'hf_token': None,\n",
      " 'int8_down_proj': False,\n",
      " 'k_asym': False,\n",
      " 'k_bits': 4,\n",
      " 'k_clip_ratio': 1.0,\n",
      " 'k_groupsize': -1,\n",
      " 'k_pre_rope': False,\n",
      " 'layer_idx': 10,\n",
      " 'learn_r1': True,\n",
      " 'learn_r2': True,\n",
      " 'lm_eval': False,\n",
      " 'lm_eval_batch_size': 128,\n",
      " 'load_qmodel_path': None,\n",
      " 'model': 'meta-llama/Llama-2-7b-hf',\n",
      " 'momentum': 0.0,\n",
      " 'nsamples': 128,\n",
      " 'percdamp': 0.01,\n",
      " 'prefix_r': '',\n",
      " 'rotate': True,\n",
      " 'rotate_mode': 'hadamard',\n",
      " 'rotation_seed': -1,\n",
      " 'save_name': '20240623_063116',\n",
      " 'save_path': '/ceph/echoi/codes/QuaRot/fake_quant/experiments/meta-llama/Llama-2-7b-hf/20240623_063116',\n",
      " 'save_qmodel_path': None,\n",
      " 'seed': 0,\n",
      " 'tasks': ['piqa',\n",
      "           'hellaswag',\n",
      "           'arc_easy',\n",
      "           'arc_challenge',\n",
      "           'winogrande',\n",
      "           'lambada'],\n",
      " 'v_asym': False,\n",
      " 'v_bits': 4,\n",
      " 'v_clip_ratio': 1.0,\n",
      " 'v_groupsize': -1,\n",
      " 'w_asym': False,\n",
      " 'w_bits': 16,\n",
      " 'w_clip': True,\n",
      " 'w_groupsize': -1,\n",
      " 'w_rtn': False,\n",
      " 'wandb': False,\n",
      " 'wandb_id': None,\n",
      " 'wandb_project': None}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = utils.parser_gen('--model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 16 --w_clip --bsz 1'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data_utils.get_loaders(\n",
    "    args.cal_dataset, nsamples=args.nsamples,\n",
    "    seed=args.seed, model=args.model,\n",
    "    seqlen=2048, eval_mode=False\n",
    ")\n",
    "\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "label_smoother = LabelSmoother(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.79it/s]\n",
      "---> Loading meta-llama/Llama-2-7b-hf Model with seq_len: 2048\n"
     ]
    }
   ],
   "source": [
    "transformers.set_seed(args.seed)\n",
    "model = model_utils.get_model(args.model, args.hf_token)\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "#utils.distribute_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rotated_llama import RotatedLlamaForCausalLM\n",
    "with torch.no_grad():\n",
    "    rotated_model = RotatedLlamaForCausalLM(model.config, model)\n",
    "del model\n",
    "rotated_model.eval()\n",
    "rotation_utils.fuse_layer_norms(rotated_model)\n",
    "for p in rotated_model.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_utils.add_actquant(\n",
    "    rotated_model,\n",
    "    layers=[nn.Linear,\n",
    "            ActQuantWrapper,\n",
    "            rotated_llama.RotatedLinear,\n",
    "            rotated_llama.RotatedOVProj]\n",
    ")\n",
    "\n",
    "qlayers = quant_utils.find_qlayers(\n",
    "    rotated_model.model,\n",
    "    layers=[nn.Linear,\n",
    "            ActQuantWrapper,\n",
    "            rotated_llama.RotatedLinear,\n",
    "            rotated_llama.RotatedOVProj]\n",
    ")\n",
    "\n",
    "for name in qlayers:\n",
    "    if 'down_proj' in name:\n",
    "        had_K, K = hadamard_utils.get_hadK(rotated_model.config.intermediate_size)\n",
    "        hadamard_utils.apply_exact_had_to_linear(qlayers[name].module, had_dim=-1, output=False)\n",
    "        qlayers[name].online_full_had = True\n",
    "        qlayers[name].had_K = had_K\n",
    "        qlayers[name].K = K\n",
    "        qlayers[name].fp32_had = args.fp32_had\n",
    "\n",
    "if args.a_bits < 16 or args.v_bits < 16:\n",
    "    qlayers = quant_utils.find_qlayers(rotated_model, layers=[quant_utils.ActQuantWrapper])\n",
    "    down_proj_groupsize = -1\n",
    "    if args.a_groupsize > 0 and \"llama\" in args.model:\n",
    "        down_proj_groupsize = utils.llama_down_proj_groupsize(rotated_model, args.a_groupsize)\n",
    "    \n",
    "    for name in qlayers:            \n",
    "        layer_input_bits = args.a_bits\n",
    "        layer_groupsize = args.a_groupsize\n",
    "        layer_a_sym = not(args.a_asym)\n",
    "        layer_a_clip = args.a_clip_ratio\n",
    "        \n",
    "        if 'v_proj' in name and args.v_bits < 16: #Set the v_proj precision\n",
    "            qlayers[name].out_quantizer.configure(bits=args.v_bits,\n",
    "                                            groupsize=args.v_groupsize,\n",
    "                                            sym=not(args.v_asym),\n",
    "                                            clip_ratio=args.v_clip_ratio)\n",
    "        \n",
    "        if 'lm_head' in name: #Skip lm_head quantization   \n",
    "            layer_input_bits = 16\n",
    "        \n",
    "        if 'down_proj' in name: #Set the down_proj precision\n",
    "            if args.int8_down_proj:\n",
    "                layer_input_bits = 8\n",
    "            layer_groupsize = down_proj_groupsize\n",
    "\n",
    "            \n",
    "        qlayers[name].quantizer.configure(bits=layer_input_bits,\n",
    "                                            groupsize=layer_groupsize,\n",
    "                                            sym=layer_a_sym,\n",
    "                                            clip_ratio=layer_a_clip)\n",
    "\n",
    "if args.w_bits < 16:\n",
    "    save_dict = {}\n",
    "    if args.load_qmodel_path: # Load Quantized Rotated Model\n",
    "        assert args.rotate, \"Model should be rotated to load a quantized model!\"\n",
    "        assert not args.save_qmodel_path, \"Cannot save a quantized model if it is already loaded!\"\n",
    "        print(\"Load quantized model from \", args.load_qmodel_path)\n",
    "        save_dict = torch.load(args.load_qmodel_path)\n",
    "        rotated_model.load_state_dict(save_dict[\"model\"])\n",
    "        \n",
    "    elif not args.w_rtn: # GPTQ Weight Quantization\n",
    "        assert \"llama\" in args.model, \"Only llama is supported for GPTQ!\"\n",
    "        \n",
    "        trainloader = data_utils.get_loaders(\n",
    "            args.cal_dataset, nsamples=args.nsamples,\n",
    "            seed=args.seed, model=args.model,\n",
    "            seqlen=model.seqlen, eval_mode=False\n",
    "        )\n",
    "        quantizers = gptq_utils.gptq_fwrd(\n",
    "            rotated_model,\n",
    "            trainloader,\n",
    "            utils.DEV,\n",
    "            args,\n",
    "            [rotated_llama.RotatedLinear, rotated_llama.RotatedOVProj])\n",
    "        save_dict[\"w_quantizers\"] = quantizers\n",
    "    else: # RTN Weight Quantization\n",
    "        quantizers = gptq_utils.rtn_fwrd(rotated_model, utils.DEV, args)\n",
    "        save_dict[\"w_quantizers\"] = quantizers\n",
    "        \n",
    "    if args.save_qmodel_path:\n",
    "        save_dict[\"model\"] = model.state_dict()\n",
    "        torch.save(save_dict, args.save_qmodel_path)\n",
    "\n",
    "if args.k_bits < 16:\n",
    "    if args.k_pre_rope:\n",
    "        raise NotImplementedError(\"Pre-RoPE quantization is not supported yet!\")\n",
    "    else:\n",
    "        rope_function_name = model_utils.get_rope_function_name(rotated_model)\n",
    "        layers = model_utils.get_layers(rotated_model)\n",
    "        k_quant_config = {'k_bits':args.k_bits, \"k_groupsize\": args.k_groupsize,\n",
    "                                        \"k_sym\": not(args.k_asym), \"k_clip_ratio\": args.k_clip_ratio}\n",
    "        for layer in layers:\n",
    "            rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(\n",
    "                        layer.self_attn, \n",
    "                        rope_function_name, \n",
    "                        config=rotated_model.config,\n",
    "                        **k_quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 50184060928, 1: 50758680576, 'cpu': 227161223168}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU memory (from distribute_model): 12.71 -> 12.71 GB (0.00 GB)\n"
     ]
    }
   ],
   "source": [
    "utils.distribute_model(rotated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = random_hadamard_matrix(rotated_model.config.hidden_size, utils.DEV).to(dtype=torch.float64)\n",
    "Q1 = nn.Parameter(q1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stiefel import stiefel_optimizer\n",
    "optimizer = stiefel_optimizer.SGDG([\n",
    "    {'params': [Q1], 'lr': 1.5, 'momentum': 0.0, 'stiefel': True}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm.tqdm(range(0 + 1, 100 + 1), desc=\"Training progress\", dynamic_ncols=True)\n",
    "idx_stack = None\n",
    "\n",
    "def lr_schedule(iter, total_iter, max_lr, min_lr):\n",
    "    return max_lr - iter / total_iter * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward 0\n",
      "Forward 1\n",
      "Forward 2\n",
      "Forward 3\n",
      "Forward 4\n",
      "Forward 5\n",
      "Forward 6\n",
      "Forward 7\n",
      "Forward 8\n",
      "Forward 9\n",
      "Forward 10\n",
      "Forward 11\n",
      "Forward 12\n",
      "Forward 13\n",
      "Forward 14\n",
      "Forward 15\n",
      "Forward 16\n",
      "Forward 17\n",
      "Forward 18\n",
      "Forward 19\n",
      "Forward 20\n",
      "Forward 21\n",
      "Forward 22\n",
      "Forward 23\n",
      "Forward 24\n",
      "Forward 25\n",
      "Forward 26\n",
      "Forward 27\n",
      "Forward 28\n",
      "Forward 29\n",
      "Forward 30\n",
      "Forward 31\n",
      "forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|          | 0/100 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#for p in [Q1]+Q2s:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#    print(p.grad is None)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[1;32m     32\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mortho1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mmatmul(Q1,\u001b[38;5;250m \u001b[39mQ1\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtorch\u001b[38;5;241m.\u001b[39meye(Q1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(Q1\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m          }\n\u001b[1;32m     37\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:379\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[0;32m--> 379\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/ceph/echoi/codes/QuaRot/fake_quant/stiefel/stiefel_optimizer.py:110\u001b[0m, in \u001b[0;36mSGDG.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m V \u001b[38;5;241m=\u001b[39m param_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    109\u001b[0m V \u001b[38;5;241m=\u001b[39m momentum \u001b[38;5;241m*\u001b[39m V \u001b[38;5;241m-\u001b[39m g\u001b[38;5;241m.\u001b[39mt()   \n\u001b[0;32m--> 110\u001b[0m MX \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m XMX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(unity, MX)\n\u001b[1;32m    112\u001b[0m XXMX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(unity\u001b[38;5;241m.\u001b[39mt(), XMX)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: float != c10::Half"
     ]
    }
   ],
   "source": [
    "for iteration in pbar:\n",
    "    if not idx_stack:\n",
    "        idx_stack = list(range(0, len(trainloader)))\n",
    "    \n",
    "    idx = idx_stack.pop(randint(0, len(idx_stack) - 1))\n",
    "    \n",
    "    data = trainloader[idx]\n",
    "    \n",
    "    input = data[0].to(utils.DEV)\n",
    "    #target = data[1]#.to(utils.DEV)\n",
    "    #print()\n",
    "    output = rotated_model(input, R1=Q1)\n",
    "    print('forward')\n",
    "    loss = label_smoother(output, input, shift_labels=True)\n",
    "    \n",
    "    Q1.retain_grad()\n",
    "    #for Q2 in Q2s:\n",
    "    #    Q2.retain_grad()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    print('backward')\n",
    "    #accelerator.backward(loss)\n",
    "    lr = lr_schedule(iteration, 100, 1.5, 0)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    #for p in [Q1]+Q2s:\n",
    "    #    print(p.grad is None)\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar.set_postfix(\n",
    "            {'CE': f'{loss.item():.3f}',\n",
    "             'ortho1': f'{(torch.matmul(Q1, Q1.T) - torch.eye(Q1.size(0)).to(Q1.device)).sum().item():.3f}',\n",
    "             #'ortho2': f'{(torch.matmul(Q2, Q2.T) - torch.eye(Q2.size(0)).to(Q2.device)).sum().item():.3f}',\n",
    "             #'det(Q)': f'{torch.linalg.det(Q):.3f}'\n",
    "             }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight False\n",
      "model.layers.0.self_attn.q_proj.weight False\n",
      "model.layers.0.self_attn.k_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.weight False\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight False\n",
      "model.layers.0.mlp.down_proj.weight False\n",
      "model.layers.0.input_layernorm.weight False\n",
      "model.layers.0.post_attention_layernorm.weight False\n",
      "model.layers.1.self_attn.q_proj.weight False\n",
      "model.layers.1.self_attn.k_proj.weight False\n",
      "model.layers.1.self_attn.v_proj.weight False\n",
      "model.layers.1.self_attn.o_proj.weight False\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight False\n",
      "model.layers.1.mlp.down_proj.weight False\n",
      "model.layers.1.input_layernorm.weight False\n",
      "model.layers.1.post_attention_layernorm.weight False\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight False\n",
      "model.layers.2.mlp.down_proj.weight False\n",
      "model.layers.2.input_layernorm.weight False\n",
      "model.layers.2.post_attention_layernorm.weight False\n",
      "model.layers.3.self_attn.q_proj.weight False\n",
      "model.layers.3.self_attn.k_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.weight False\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight False\n",
      "model.layers.3.mlp.down_proj.weight False\n",
      "model.layers.3.input_layernorm.weight False\n",
      "model.layers.3.post_attention_layernorm.weight False\n",
      "model.layers.4.self_attn.q_proj.weight False\n",
      "model.layers.4.self_attn.k_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.weight False\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight False\n",
      "model.layers.4.mlp.down_proj.weight False\n",
      "model.layers.4.input_layernorm.weight False\n",
      "model.layers.4.post_attention_layernorm.weight False\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight False\n",
      "model.layers.5.mlp.down_proj.weight False\n",
      "model.layers.5.input_layernorm.weight False\n",
      "model.layers.5.post_attention_layernorm.weight False\n",
      "model.layers.6.self_attn.q_proj.weight False\n",
      "model.layers.6.self_attn.k_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.weight False\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight False\n",
      "model.layers.6.mlp.down_proj.weight False\n",
      "model.layers.6.input_layernorm.weight False\n",
      "model.layers.6.post_attention_layernorm.weight False\n",
      "model.layers.7.self_attn.q_proj.weight False\n",
      "model.layers.7.self_attn.k_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.weight False\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight False\n",
      "model.layers.7.mlp.down_proj.weight False\n",
      "model.layers.7.input_layernorm.weight False\n",
      "model.layers.7.post_attention_layernorm.weight False\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight False\n",
      "model.layers.8.mlp.down_proj.weight False\n",
      "model.layers.8.input_layernorm.weight False\n",
      "model.layers.8.post_attention_layernorm.weight False\n",
      "model.layers.9.self_attn.q_proj.weight False\n",
      "model.layers.9.self_attn.k_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.weight False\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight False\n",
      "model.layers.9.mlp.down_proj.weight False\n",
      "model.layers.9.input_layernorm.weight False\n",
      "model.layers.9.post_attention_layernorm.weight False\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight False\n",
      "model.layers.10.mlp.down_proj.weight False\n",
      "model.layers.10.input_layernorm.weight False\n",
      "model.layers.10.post_attention_layernorm.weight False\n",
      "model.layers.11.self_attn.q_proj.weight False\n",
      "model.layers.11.self_attn.k_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.weight False\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight False\n",
      "model.layers.11.mlp.down_proj.weight False\n",
      "model.layers.11.input_layernorm.weight False\n",
      "model.layers.11.post_attention_layernorm.weight False\n",
      "model.layers.12.self_attn.q_proj.weight False\n",
      "model.layers.12.self_attn.k_proj.weight False\n",
      "model.layers.12.self_attn.v_proj.weight False\n",
      "model.layers.12.self_attn.o_proj.weight False\n",
      "model.layers.12.mlp.gate_proj.weight False\n",
      "model.layers.12.mlp.up_proj.weight False\n",
      "model.layers.12.mlp.down_proj.weight False\n",
      "model.layers.12.input_layernorm.weight False\n",
      "model.layers.12.post_attention_layernorm.weight False\n",
      "model.layers.13.self_attn.q_proj.weight False\n",
      "model.layers.13.self_attn.k_proj.weight False\n",
      "model.layers.13.self_attn.v_proj.weight False\n",
      "model.layers.13.self_attn.o_proj.weight False\n",
      "model.layers.13.mlp.gate_proj.weight False\n",
      "model.layers.13.mlp.up_proj.weight False\n",
      "model.layers.13.mlp.down_proj.weight False\n",
      "model.layers.13.input_layernorm.weight False\n",
      "model.layers.13.post_attention_layernorm.weight False\n",
      "model.layers.14.self_attn.q_proj.weight False\n",
      "model.layers.14.self_attn.k_proj.weight False\n",
      "model.layers.14.self_attn.v_proj.weight False\n",
      "model.layers.14.self_attn.o_proj.weight False\n",
      "model.layers.14.mlp.gate_proj.weight False\n",
      "model.layers.14.mlp.up_proj.weight False\n",
      "model.layers.14.mlp.down_proj.weight False\n",
      "model.layers.14.input_layernorm.weight False\n",
      "model.layers.14.post_attention_layernorm.weight False\n",
      "model.layers.15.self_attn.q_proj.weight False\n",
      "model.layers.15.self_attn.k_proj.weight False\n",
      "model.layers.15.self_attn.v_proj.weight False\n",
      "model.layers.15.self_attn.o_proj.weight False\n",
      "model.layers.15.mlp.gate_proj.weight False\n",
      "model.layers.15.mlp.up_proj.weight False\n",
      "model.layers.15.mlp.down_proj.weight False\n",
      "model.layers.15.input_layernorm.weight False\n",
      "model.layers.15.post_attention_layernorm.weight False\n",
      "model.layers.16.self_attn.q_proj.weight False\n",
      "model.layers.16.self_attn.k_proj.weight False\n",
      "model.layers.16.self_attn.v_proj.weight False\n",
      "model.layers.16.self_attn.o_proj.weight False\n",
      "model.layers.16.mlp.gate_proj.weight False\n",
      "model.layers.16.mlp.up_proj.weight False\n",
      "model.layers.16.mlp.down_proj.weight False\n",
      "model.layers.16.input_layernorm.weight False\n",
      "model.layers.16.post_attention_layernorm.weight False\n",
      "model.layers.17.self_attn.q_proj.weight False\n",
      "model.layers.17.self_attn.k_proj.weight False\n",
      "model.layers.17.self_attn.v_proj.weight False\n",
      "model.layers.17.self_attn.o_proj.weight False\n",
      "model.layers.17.mlp.gate_proj.weight False\n",
      "model.layers.17.mlp.up_proj.weight False\n",
      "model.layers.17.mlp.down_proj.weight False\n",
      "model.layers.17.input_layernorm.weight False\n",
      "model.layers.17.post_attention_layernorm.weight False\n",
      "model.layers.18.self_attn.q_proj.weight False\n",
      "model.layers.18.self_attn.k_proj.weight False\n",
      "model.layers.18.self_attn.v_proj.weight False\n",
      "model.layers.18.self_attn.o_proj.weight False\n",
      "model.layers.18.mlp.gate_proj.weight False\n",
      "model.layers.18.mlp.up_proj.weight False\n",
      "model.layers.18.mlp.down_proj.weight False\n",
      "model.layers.18.input_layernorm.weight False\n",
      "model.layers.18.post_attention_layernorm.weight False\n",
      "model.layers.19.self_attn.q_proj.weight False\n",
      "model.layers.19.self_attn.k_proj.weight False\n",
      "model.layers.19.self_attn.v_proj.weight False\n",
      "model.layers.19.self_attn.o_proj.weight False\n",
      "model.layers.19.mlp.gate_proj.weight False\n",
      "model.layers.19.mlp.up_proj.weight False\n",
      "model.layers.19.mlp.down_proj.weight False\n",
      "model.layers.19.input_layernorm.weight False\n",
      "model.layers.19.post_attention_layernorm.weight False\n",
      "model.layers.20.self_attn.q_proj.weight False\n",
      "model.layers.20.self_attn.k_proj.weight False\n",
      "model.layers.20.self_attn.v_proj.weight False\n",
      "model.layers.20.self_attn.o_proj.weight False\n",
      "model.layers.20.mlp.gate_proj.weight False\n",
      "model.layers.20.mlp.up_proj.weight False\n",
      "model.layers.20.mlp.down_proj.weight False\n",
      "model.layers.20.input_layernorm.weight False\n",
      "model.layers.20.post_attention_layernorm.weight False\n",
      "model.layers.21.self_attn.q_proj.weight False\n",
      "model.layers.21.self_attn.k_proj.weight False\n",
      "model.layers.21.self_attn.v_proj.weight False\n",
      "model.layers.21.self_attn.o_proj.weight False\n",
      "model.layers.21.mlp.gate_proj.weight False\n",
      "model.layers.21.mlp.up_proj.weight False\n",
      "model.layers.21.mlp.down_proj.weight False\n",
      "model.layers.21.input_layernorm.weight False\n",
      "model.layers.21.post_attention_layernorm.weight False\n",
      "model.layers.22.self_attn.q_proj.weight False\n",
      "model.layers.22.self_attn.k_proj.weight False\n",
      "model.layers.22.self_attn.v_proj.weight False\n",
      "model.layers.22.self_attn.o_proj.weight False\n",
      "model.layers.22.mlp.gate_proj.weight False\n",
      "model.layers.22.mlp.up_proj.weight False\n",
      "model.layers.22.mlp.down_proj.weight False\n",
      "model.layers.22.input_layernorm.weight False\n",
      "model.layers.22.post_attention_layernorm.weight False\n",
      "model.layers.23.self_attn.q_proj.weight False\n",
      "model.layers.23.self_attn.k_proj.weight False\n",
      "model.layers.23.self_attn.v_proj.weight False\n",
      "model.layers.23.self_attn.o_proj.weight False\n",
      "model.layers.23.mlp.gate_proj.weight False\n",
      "model.layers.23.mlp.up_proj.weight False\n",
      "model.layers.23.mlp.down_proj.weight False\n",
      "model.layers.23.input_layernorm.weight False\n",
      "model.layers.23.post_attention_layernorm.weight False\n",
      "model.layers.24.self_attn.q_proj.weight False\n",
      "model.layers.24.self_attn.k_proj.weight False\n",
      "model.layers.24.self_attn.v_proj.weight False\n",
      "model.layers.24.self_attn.o_proj.weight False\n",
      "model.layers.24.mlp.gate_proj.weight False\n",
      "model.layers.24.mlp.up_proj.weight False\n",
      "model.layers.24.mlp.down_proj.weight False\n",
      "model.layers.24.input_layernorm.weight False\n",
      "model.layers.24.post_attention_layernorm.weight False\n",
      "model.layers.25.self_attn.q_proj.weight False\n",
      "model.layers.25.self_attn.k_proj.weight False\n",
      "model.layers.25.self_attn.v_proj.weight False\n",
      "model.layers.25.self_attn.o_proj.weight False\n",
      "model.layers.25.mlp.gate_proj.weight False\n",
      "model.layers.25.mlp.up_proj.weight False\n",
      "model.layers.25.mlp.down_proj.weight False\n",
      "model.layers.25.input_layernorm.weight False\n",
      "model.layers.25.post_attention_layernorm.weight False\n",
      "model.layers.26.self_attn.q_proj.weight False\n",
      "model.layers.26.self_attn.k_proj.weight False\n",
      "model.layers.26.self_attn.v_proj.weight False\n",
      "model.layers.26.self_attn.o_proj.weight False\n",
      "model.layers.26.mlp.gate_proj.weight False\n",
      "model.layers.26.mlp.up_proj.weight False\n",
      "model.layers.26.mlp.down_proj.weight False\n",
      "model.layers.26.input_layernorm.weight False\n",
      "model.layers.26.post_attention_layernorm.weight False\n",
      "model.layers.27.self_attn.q_proj.weight False\n",
      "model.layers.27.self_attn.k_proj.weight False\n",
      "model.layers.27.self_attn.v_proj.weight False\n",
      "model.layers.27.self_attn.o_proj.weight False\n",
      "model.layers.27.mlp.gate_proj.weight False\n",
      "model.layers.27.mlp.up_proj.weight False\n",
      "model.layers.27.mlp.down_proj.weight False\n",
      "model.layers.27.input_layernorm.weight False\n",
      "model.layers.27.post_attention_layernorm.weight False\n",
      "model.layers.28.self_attn.q_proj.weight False\n",
      "model.layers.28.self_attn.k_proj.weight False\n",
      "model.layers.28.self_attn.v_proj.weight False\n",
      "model.layers.28.self_attn.o_proj.weight False\n",
      "model.layers.28.mlp.gate_proj.weight False\n",
      "model.layers.28.mlp.up_proj.weight False\n",
      "model.layers.28.mlp.down_proj.weight False\n",
      "model.layers.28.input_layernorm.weight False\n",
      "model.layers.28.post_attention_layernorm.weight False\n",
      "model.layers.29.self_attn.q_proj.weight False\n",
      "model.layers.29.self_attn.k_proj.weight False\n",
      "model.layers.29.self_attn.v_proj.weight False\n",
      "model.layers.29.self_attn.o_proj.weight False\n",
      "model.layers.29.mlp.gate_proj.weight False\n",
      "model.layers.29.mlp.up_proj.weight False\n",
      "model.layers.29.mlp.down_proj.weight False\n",
      "model.layers.29.input_layernorm.weight False\n",
      "model.layers.29.post_attention_layernorm.weight False\n",
      "model.layers.30.self_attn.q_proj.weight False\n",
      "model.layers.30.self_attn.k_proj.weight False\n",
      "model.layers.30.self_attn.v_proj.weight False\n",
      "model.layers.30.self_attn.o_proj.weight False\n",
      "model.layers.30.mlp.gate_proj.weight False\n",
      "model.layers.30.mlp.up_proj.weight False\n",
      "model.layers.30.mlp.down_proj.weight False\n",
      "model.layers.30.input_layernorm.weight False\n",
      "model.layers.30.post_attention_layernorm.weight False\n",
      "model.layers.31.self_attn.q_proj.weight False\n",
      "model.layers.31.self_attn.k_proj.weight False\n",
      "model.layers.31.self_attn.v_proj.weight False\n",
      "model.layers.31.self_attn.o_proj.weight False\n",
      "model.layers.31.mlp.gate_proj.weight False\n",
      "model.layers.31.mlp.up_proj.weight False\n",
      "model.layers.31.mlp.down_proj.weight False\n",
      "model.layers.31.input_layernorm.weight False\n",
      "model.layers.31.post_attention_layernorm.weight False\n",
      "model.norm.weight False\n",
      "lm_head.weight False\n"
     ]
    }
   ],
   "source": [
    "for n, p in rotated_model.named_parameters():\n",
    "    print(n, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
