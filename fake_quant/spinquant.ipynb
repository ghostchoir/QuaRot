{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\"\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rotation_utils import random_orthogonal_matrix\n",
    "from hadamard_utils import random_hadamard_matrix, apply_exact_had_to_linear\n",
    "from quant_utils import ActQuantWrapper\n",
    "\n",
    "import utils\n",
    "import model_utils\n",
    "import data_utils\n",
    "import transformers\n",
    "import quant_utils\n",
    "import rotation_utils\n",
    "import gptq_utils\n",
    "import eval_utils\n",
    "import hadamard_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatedEmbedding(nn.Embedding):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        padding_idx=None,\n",
    "        max_norm=None,\n",
    "        norm_type=2.0,\n",
    "        scale_grad_by_freq=False,\n",
    "        sparse=False,\n",
    "        _weight=None,\n",
    "        _freeze=False,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx,\n",
    "                         max_norm, norm_type, scale_grad_by_freq, sparse,\n",
    "                         _weight, _freeze, device, dtype)\n",
    "        \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W = self.weight\n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(W.to(dtype=self.Q.dtype), self.Q.to(W.device)).to(dtype=W.dtype)\n",
    "            #print('emb')\n",
    "            #print(W_.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "            \n",
    "        return F.embedding(\n",
    "            x, W_,\n",
    "            self.padding_idx,\n",
    "            self.max_norm,\n",
    "            self.norm_type,\n",
    "            self.scale_grad_by_freq,\n",
    "            self.sparse)\n",
    "\n",
    "class RotatedHead(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        W = self.weight\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(W.to(dtype=self.Q.dtype), self.Q.to(W.device)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('head')\n",
    "            #print(W_.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_,\n",
    "        )\n",
    "\n",
    "class RotatedLinearIn(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        W = self.weight\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(W.to(dtype=self.Q.dtype), self.Q.to(W.device)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('linear in')\n",
    "            #print(W_.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_,\n",
    "        )\n",
    "\n",
    "\n",
    "class RotatedOVProj(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Qin=None,\n",
    "        Qout=None,\n",
    "        output=False,\n",
    "        nheads=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Qin is not None:\n",
    "            self.register_buffer(\"Qin\", Qin)\n",
    "        else:\n",
    "            self.Qin = None\n",
    "        \n",
    "        if Qout is not None:\n",
    "            self.register_buffer(\"Qout\", Qout)\n",
    "        else:\n",
    "            self.Qout = None\n",
    "        \n",
    "        self.output = output\n",
    "        self.nheads = nheads\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W = self.weight\n",
    "        \n",
    "        if self.Qin is not None:\n",
    "            if self.output:\n",
    "                W_ = torch.matmul(W.to(dtype=self.Qin.dtype), self.Qin.to(W.device)).to(dtype=W.dtype)\n",
    "            else:\n",
    "                W_ = W.to(dtype=self.Qin.dtype).reshape(W.size(0), self.nheads, -1)\n",
    "                W_ = torch.einsum('inh,hj->inj', W_, self.Qin.to(W.device)).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear o')\n",
    "                #print(W_.grad_fn)\n",
    "        else:\n",
    "            W_ = W\n",
    "        \n",
    "        if self.Qout is not None:\n",
    "            if self.output:\n",
    "                W_ = W_.to(dtype=self.Qout.dtype).reshape(self.nheads, -1, W.size(1))\n",
    "                W_ = torch.einsum('ih,nhj->nij', self.Qout.to(W.device).T, W_).reshape(W.size(0), -1).to(dtype=W.dtype)\n",
    "                \n",
    "                #print('linear v')\n",
    "                #print(W_.grad_fn)\n",
    "            else:\n",
    "                W_ = torch.matmul(self.Qout.to(W.device).T, W_.to(dtype=self.Qout.dtype)).to(dtype=W.dtype)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_,\n",
    "        )\n",
    "\n",
    "\n",
    "class RotatedLinearOut(nn.Linear):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        Q=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "    \n",
    "        if Q is not None:\n",
    "            self.register_buffer(\"Q\", Q)\n",
    "        else:\n",
    "            self.Q = None\n",
    "    \n",
    "    def forward(self, x, Q=None):\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            W_ = torch.matmul(self.Q.to(W.device).T, W.to(dtype=self.Q.dtype)).to(dtype=W.dtype)\n",
    "            \n",
    "            #print('linear out')\n",
    "            #print(W_.grad_fn)\n",
    "            if b is not None:\n",
    "                b_ = torch.matmul(self.Q.to(W.device).T, b.to(dtype=self.Q.dtype)).to(dtype=b.dtype)\n",
    "            else:\n",
    "                b_ = b\n",
    "        else:\n",
    "            W_ = W\n",
    "            b_ = b\n",
    "        \n",
    "        return F.linear(\n",
    "            x, W_, b_\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_embeddings(model, Q):\n",
    "    \n",
    "    original_emb = model.model.embed_tokens\n",
    "    \n",
    "    new_emb = RotatedEmbedding(\n",
    "        original_emb.num_embeddings,\n",
    "        original_emb.embedding_dim,\n",
    "        original_emb.padding_idx,\n",
    "        original_emb.max_norm,\n",
    "        original_emb.norm_type,\n",
    "        original_emb.scale_grad_by_freq,\n",
    "        original_emb.sparse,\n",
    "        original_emb.weight.data,\n",
    "        not original_emb.weight.requires_grad,\n",
    "        original_emb.weight.data.device,\n",
    "        original_emb.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    setattr(model.model, 'embed_tokens', new_emb)\n",
    "\n",
    "\n",
    "def rotate_attention_inputs(layer, Q) -> None:\n",
    "    # Rotate the WQ, WK and WV matrices of the self-attention layer.\n",
    "    for name in ['q_proj', 'k_proj']:#, 'v_proj']:\n",
    "        original_matrix = getattr(layer.self_attn, name)\n",
    "        \n",
    "        new_matrix = RotatedLinearIn(\n",
    "            original_matrix.in_features,\n",
    "            original_matrix.out_features,\n",
    "            original_matrix.bias is not None,\n",
    "            original_matrix.weight.data.device,\n",
    "            original_matrix.weight.data.dtype,\n",
    "            Q\n",
    "        )\n",
    "        \n",
    "        new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "        if original_matrix.bias is not None:\n",
    "            new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "        \n",
    "        setattr(layer.self_attn, name, new_matrix)\n",
    "        del original_matrix\n",
    "\n",
    "\n",
    "def rotate_attention_output(layer, Q) -> None:\n",
    "    # Rotate output matrix of the self-attention layer.\n",
    "    original_matrix = layer.self_attn.o_proj\n",
    "    \n",
    "    new_matrix = RotatedLinearOut(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.self_attn, 'o_proj', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_mlp_input(layer, Q):\n",
    "    # Rotate the MLP input weights.\n",
    "    \n",
    "    for name in ['up_proj', 'gate_proj']:\n",
    "        original_matrix = getattr(layer.mlp, name)\n",
    "        \n",
    "        new_matrix = RotatedLinearIn(\n",
    "            original_matrix.in_features,\n",
    "            original_matrix.out_features,\n",
    "            original_matrix.bias is not None,\n",
    "            original_matrix.weight.data.device,\n",
    "            original_matrix.weight.data.dtype,\n",
    "            Q\n",
    "        )\n",
    "        \n",
    "        new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "        if original_matrix.bias is not None:\n",
    "            new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "        \n",
    "        setattr(layer.mlp, name, new_matrix)\n",
    "        del original_matrix\n",
    "\n",
    "\n",
    "def rotate_mlp_output(layer, Q):\n",
    "    # Rotate the MLP output weights and bias.\n",
    "    original_matrix = layer.mlp.down_proj\n",
    "    \n",
    "    new_matrix = RotatedLinearOut(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.mlp, 'down_proj', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_head(model, Q: torch.Tensor) -> None:\n",
    "    # Rotate the head.\n",
    "    original_matrix = model.lm_head\n",
    "    \n",
    "    new_matrix = RotatedLinearIn(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(model, 'lm_head', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_ov_proj(layer, Q1, Q2, nheads):\n",
    "    #print(nheads)\n",
    "    original_matrix = layer.self_attn.o_proj\n",
    "    \n",
    "    new_matrix = RotatedOVProj(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q2, Q1, False, nheads\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.self_attn, 'o_proj', new_matrix)\n",
    "    del original_matrix\n",
    "    \n",
    "    original_matrix = layer.self_attn.v_proj\n",
    "    \n",
    "    new_matrix = RotatedOVProj(\n",
    "        original_matrix.in_features,\n",
    "        original_matrix.out_features,\n",
    "        original_matrix.bias is not None,\n",
    "        original_matrix.weight.data.device,\n",
    "        original_matrix.weight.data.dtype,\n",
    "        Q1, Q2, True, nheads\n",
    "    )\n",
    "    \n",
    "    new_matrix.weight.data = original_matrix.weight.data.clone()\n",
    "    if original_matrix.bias is not None:\n",
    "        new_matrix.bias.data = original_matrix.bias.data.clone()\n",
    "    \n",
    "    setattr(layer.self_attn, 'v_proj', new_matrix)\n",
    "    del original_matrix\n",
    "\n",
    "\n",
    "def rotate_model(model, args):\n",
    "    #q = random_orthogonal_matrix(model.config.hidden_size, utils.DEV).to(dtype=torch.float32)\n",
    "    \n",
    "    config = model.config\n",
    "    num_heads = config.num_attention_heads\n",
    "    model_dim = config.hidden_size\n",
    "    head_dim = model_dim // num_heads\n",
    "    \n",
    "    q1 = random_hadamard_matrix(model.config.hidden_size, utils.DEV).to(dtype=torch.float32)\n",
    "    Q1 = nn.Parameter(q1, requires_grad=True)\n",
    "    Q2s = []\n",
    "    #q2 = random_hadamard_matrix(head_dim, utils.DEV).to(dtype=torch.float32)\n",
    "    #Q2 = nn.Parameter(q2, requires_grad=True)\n",
    "    \n",
    "    model_type = model_utils.model_type_extractor(model)\n",
    "    rotate_embeddings(model, Q1)\n",
    "    rotate_head(model, Q1)\n",
    "    utils.cleanup_memory()\n",
    "    layers = model_utils.get_transformer_layers(model, \n",
    "                                                model_type=model_type)\n",
    "    \n",
    "    for idx, layer in enumerate(tqdm.tqdm(layers, unit=\"layer\", desc=\"Rotating\")):\n",
    "        q2 = random_hadamard_matrix(head_dim, layers[idx].self_attn.v_proj.weight.device).to(dtype=torch.float32)#.clone().detach().requires_grad_(True)\n",
    "        Q2 = nn.Parameter(q2, requires_grad=True)\n",
    "        rotate_attention_inputs(layers[idx], Q1)\n",
    "        #rotate_attention_output(layers[idx], Q1)\n",
    "        rotate_mlp_input(layers[idx], Q1)\n",
    "        rotate_mlp_output(layers[idx], Q1)\n",
    "        rotate_ov_proj(layers[idx], Q1, Q2, num_heads)\n",
    "        \n",
    "        Q2s.append(Q2)\n",
    "        #print(str(idx) + '-----------')\n",
    "        #print(layer.self_attn.v_proj)\n",
    "        #print(layer.self_attn.o_proj)\n",
    "    return Q1, Q2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "{'a_asym': False,\n",
      " 'a_bits': 4,\n",
      " 'a_clip_ratio': 1.0,\n",
      " 'a_groupsize': -1,\n",
      " 'act_order': False,\n",
      " 'bsz': 1,\n",
      " 'cal_dataset': 'wikitext2',\n",
      " 'capture_layer_io': False,\n",
      " 'distribute': False,\n",
      " 'eval_dataset': 'wikitext2',\n",
      " 'fp32_had': False,\n",
      " 'hf_token': None,\n",
      " 'int8_down_proj': False,\n",
      " 'k_asym': False,\n",
      " 'k_bits': 4,\n",
      " 'k_clip_ratio': 1.0,\n",
      " 'k_groupsize': -1,\n",
      " 'k_pre_rope': False,\n",
      " 'layer_idx': 10,\n",
      " 'lm_eval': False,\n",
      " 'lm_eval_batch_size': 128,\n",
      " 'load_qmodel_path': None,\n",
      " 'model': 'meta-llama/Llama-2-7b-hf',\n",
      " 'nsamples': 128,\n",
      " 'percdamp': 0.01,\n",
      " 'rotate': True,\n",
      " 'rotate_mode': 'hadamard',\n",
      " 'rotation_seed': -1,\n",
      " 'save_name': '20240616_080744',\n",
      " 'save_path': '/ceph/echoi/codes/QuaRot/fake_quant/experiments/meta-llama/Llama-2-7b-hf/20240616_080744',\n",
      " 'save_qmodel_path': None,\n",
      " 'seed': 0,\n",
      " 'tasks': ['piqa',\n",
      "           'hellaswag',\n",
      "           'arc_easy',\n",
      "           'arc_challenge',\n",
      "           'winogrande',\n",
      "           'lambada'],\n",
      " 'v_asym': False,\n",
      " 'v_bits': 4,\n",
      " 'v_clip_ratio': 1.0,\n",
      " 'v_groupsize': -1,\n",
      " 'w_asym': False,\n",
      " 'w_bits': 4,\n",
      " 'w_clip': True,\n",
      " 'w_groupsize': -1,\n",
      " 'w_rtn': True,\n",
      " 'wandb': False,\n",
      " 'wandb_id': None,\n",
      " 'wandb_project': None}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = utils.parser_gen('--model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --bsz 1 --w_rtn'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.99it/s]\n",
      "---> Loading meta-llama/Llama-2-7b-hf Model with seq_len: 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.set_seed(args.seed)\n",
    "model = model_utils.get_model(args.model, args.hf_token)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU memory (from <module>): 0.00 -> 0.00 GB (0.00 GB)\n"
     ]
    }
   ],
   "source": [
    "rotation_utils.fuse_layer_norms(model)\n",
    "utils.cleanup_memory(verbos=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU memory (from rotate_model): 0.19 -> 0.06 GB (-0.12 GB)\n",
      "Rotating: 100%|██████████| 32/32 [00:48<00:00,  1.50s/layer]\n",
      "GPU memory (from <module>): 0.06 -> 0.06 GB (0.00 GB)\n",
      "(RtN Quant.) Layers: 100%|██████████| 32/32 [00:25<00:00,  1.26it/s]\n",
      "GPU memory (from rtn_fwrd): 0.06 -> 0.06 GB (0.00 GB)\n",
      "GPU memory (from distribute_model): 26.75 -> 26.75 GB (0.00 GB)\n"
     ]
    }
   ],
   "source": [
    "Q1, Q2s = rotate_model(model, args)\n",
    "utils.cleanup_memory(verbos=True)\n",
    "\n",
    "quant_utils.add_actquant(\n",
    "    model,\n",
    "    layers=[nn.Linear,\n",
    "            ActQuantWrapper,\n",
    "            RotatedHead,\n",
    "            RotatedLinearIn,\n",
    "            RotatedLinearOut,\n",
    "            RotatedOVProj]\n",
    ")\n",
    "\n",
    "qlayers = quant_utils.find_qlayers(\n",
    "    model.model,\n",
    "    [nn.Linear,\n",
    "     ActQuantWrapper,\n",
    "     RotatedHead,\n",
    "     RotatedLinearIn,\n",
    "     RotatedLinearOut,\n",
    "     RotatedOVProj])\n",
    "\n",
    "for name in qlayers:\n",
    "    if 'down_proj' in name:\n",
    "        had_K, K = hadamard_utils.get_hadK(model.config.intermediate_size)\n",
    "        qlayers[name].online_full_had = True\n",
    "        qlayers[name].had_K = had_K\n",
    "        qlayers[name].K = K\n",
    "        qlayers[name].fp32_had = args.fp32_had\n",
    "    # if 'o_proj' in name:\n",
    "    #     had_K, K = hadamard_utils.get_hadK(model.config.num_attention_heads)\n",
    "    #     qlayers[name].online_partial_had = True\n",
    "    #     qlayers[name].had_K = had_K\n",
    "    #     qlayers[name].K = K\n",
    "    #     qlayers[name].had_dim = model.config.hidden_size//model.config.num_attention_heads\n",
    "    #     qlayers[name].fp32_had = args.fp32_had\n",
    "\n",
    "if args.w_bits < 16:\n",
    "    save_dict = {}\n",
    "    if args.load_qmodel_path: # Load Quantized Rotated Model\n",
    "        assert args.rotate, \"Model should be rotated to load a quantized model!\"\n",
    "        assert not args.save_qmodel_path, \"Cannot save a quantized model if it is already loaded!\"\n",
    "        print(\"Load quantized model from \", args.load_qmodel_path)\n",
    "        save_dict = torch.load(args.load_qmodel_path)\n",
    "        model.load_state_dict(save_dict[\"model\"])\n",
    "        \n",
    "    elif not args.w_rtn: # GPTQ Weight Quantization\n",
    "        assert \"llama\" in args.model, \"Only llama is supported for GPTQ!\"\n",
    "        \n",
    "        trainloader = data_utils.get_loaders(\n",
    "            args.cal_dataset, nsamples=args.nsamples,\n",
    "            seed=args.seed, model=args.model,\n",
    "            seqlen=model.seqlen, eval_mode=False\n",
    "        )\n",
    "        quantizers = gptq_utils.gptq_fwrd(\n",
    "            model,\n",
    "            trainloader,\n",
    "            utils.DEV,\n",
    "            args,\n",
    "            [RotatedHead, RotatedLinearIn, RotatedLinearOut, RotatedOVProj])\n",
    "        save_dict[\"w_quantizers\"] = quantizers\n",
    "    else: # RTN Weight Quantization\n",
    "        quantizers = gptq_utils.rtn_fwrd(model, utils.DEV, args)\n",
    "        save_dict[\"w_quantizers\"] = quantizers\n",
    "        \n",
    "    if args.save_qmodel_path:\n",
    "        save_dict[\"model\"] = model.state_dict()\n",
    "        torch.save(save_dict, args.save_qmodel_path)\n",
    "\n",
    "if args.a_bits < 16 or args.v_bits < 16:\n",
    "    qlayers = quant_utils.find_qlayers(model, layers=[quant_utils.ActQuantWrapper])\n",
    "    down_proj_groupsize = -1\n",
    "    if args.a_groupsize > 0 and \"llama\" in args.model:\n",
    "        down_proj_groupsize = utils.llama_down_proj_groupsize(model, args.a_groupsize)\n",
    "    \n",
    "    for name in qlayers:            \n",
    "        layer_input_bits = args.a_bits\n",
    "        layer_groupsize = args.a_groupsize\n",
    "        layer_a_sym = not(args.a_asym)\n",
    "        layer_a_clip = args.a_clip_ratio\n",
    "        \n",
    "        if 'v_proj' in name and args.v_bits < 16: #Set the v_proj precision\n",
    "            qlayers[name].out_quantizer.configure(bits=args.v_bits,\n",
    "                                            groupsize=args.v_groupsize,\n",
    "                                            sym=not(args.v_asym),\n",
    "                                            clip_ratio=args.v_clip_ratio)\n",
    "        \n",
    "        if 'lm_head' in name: #Skip lm_head quantization   \n",
    "            layer_input_bits = 16\n",
    "        \n",
    "        if 'down_proj' in name: #Set the down_proj precision\n",
    "            if args.int8_down_proj:\n",
    "                layer_input_bits = 8\n",
    "            layer_groupsize = down_proj_groupsize\n",
    "\n",
    "            \n",
    "        qlayers[name].quantizer.configure(bits=layer_input_bits,\n",
    "                                            groupsize=layer_groupsize,\n",
    "                                            sym=layer_a_sym,\n",
    "                                            clip_ratio=layer_a_clip)\n",
    "\n",
    "if args.k_bits < 16:\n",
    "    if args.k_pre_rope:\n",
    "        raise NotImplementedError(\"Pre-RoPE quantization is not supported yet!\")\n",
    "    else:\n",
    "        rope_function_name = model_utils.get_rope_function_name(model)\n",
    "        layers = model_utils.get_layers(model)\n",
    "        k_quant_config = {'k_bits':args.k_bits, \"k_groupsize\": args.k_groupsize,\n",
    "                                        \"k_sym\": not(args.k_asym), \"k_clip_ratio\": args.k_clip_ratio}\n",
    "        for layer in layers:\n",
    "            rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(\n",
    "                        layer.self_attn, \n",
    "                        rope_function_name, \n",
    "                        config=model.config,\n",
    "                        **k_quant_config)\n",
    "# rope_function_name = model_utils.get_rope_function_name(model)\n",
    "# layers = model_utils.get_layers(model)\n",
    "# k_quant_config = {'k_bits': 16, \"k_groupsize\": args.k_groupsize,\n",
    "#                     \"k_sym\": not(args.k_asym), \"k_clip_ratio\": args.k_clip_ratio}\n",
    "\n",
    "# for idx, layer in enumerate(layers):\n",
    "#     print('Wrapping QK', idx)\n",
    "#     rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(\n",
    "#                 layer.self_attn, \n",
    "#                 rope_function_name, \n",
    "#                 config=model.config,\n",
    "#                 **k_quant_config)\n",
    "\n",
    "utils.distribute_model(model)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "Q1.requires_grad=True\n",
    "for Q2 in Q2s:\n",
    "    Q2.requires_grad=True\n",
    "\n",
    "#optimizer = torch.optim.Adam([Q1, Q2], lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): RotatedEmbedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): ActQuantWrapper(\n",
       "            Input Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            Output Quantizer Bits: 16\n",
       "            (module): RotatedLinearIn(in_features=4096, out_features=4096, bias=False)\n",
       "            (quantizer): ActQuantizer()\n",
       "            (out_quantizer): ActQuantizer()\n",
       "          )\n",
       "          (k_proj): ActQuantWrapper(\n",
       "            Input Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            Output Quantizer Bits: 16\n",
       "            (module): RotatedLinearIn(in_features=4096, out_features=4096, bias=False)\n",
       "            (quantizer): ActQuantizer()\n",
       "            (out_quantizer): ActQuantizer()\n",
       "          )\n",
       "          (v_proj): ActQuantWrapper(\n",
       "            Input Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            Output Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            (module): RotatedOVProj(in_features=4096, out_features=4096, bias=False)\n",
       "            (quantizer): ActQuantizer()\n",
       "            (out_quantizer): ActQuantizer()\n",
       "          )\n",
       "          (o_proj): ActQuantWrapper(\n",
       "            Input Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            Output Quantizer Bits: 16\n",
       "            (module): RotatedOVProj(in_features=4096, out_features=4096, bias=False)\n",
       "            (quantizer): ActQuantizer()\n",
       "            (out_quantizer): ActQuantizer()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (apply_rotary_pos_emb_qk_rotation_wrapper): QKRotationWrapper(\n",
       "            (k_quantizer): ActQuantizer()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): ActQuantWrapper(\n",
       "            Input Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            Output Quantizer Bits: 16\n",
       "            (module): RotatedLinearIn(in_features=4096, out_features=11008, bias=False)\n",
       "            (quantizer): ActQuantizer()\n",
       "            (out_quantizer): ActQuantizer()\n",
       "          )\n",
       "          (up_proj): ActQuantWrapper(\n",
       "            Input Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            Output Quantizer Bits: 16\n",
       "            (module): RotatedLinearIn(in_features=4096, out_features=11008, bias=False)\n",
       "            (quantizer): ActQuantizer()\n",
       "            (out_quantizer): ActQuantizer()\n",
       "          )\n",
       "          (down_proj): ActQuantWrapper(\n",
       "            Input Quantizer Bits: 4 (Symmetric Per-Token)\n",
       "            Output Quantizer Bits: 16\n",
       "            (module): RotatedLinearOut(in_features=11008, out_features=4096, bias=False)\n",
       "            (quantizer): ActQuantizer()\n",
       "            (out_quantizer): ActQuantizer()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): RMSN()\n",
       "        (post_attention_layernorm): RMSN()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSN()\n",
       "  )\n",
       "  (lm_head): ActQuantWrapper(\n",
       "    Input Quantizer Bits: 16\n",
       "    Output Quantizer Bits: 16\n",
       "    (module): RotatedLinearIn(in_features=4096, out_features=32000, bias=False)\n",
       "    (quantizer): ActQuantizer()\n",
       "    (out_quantizer): ActQuantizer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data_utils.get_loaders(\n",
    "    args.cal_dataset, nsamples=args.nsamples,\n",
    "    seed=args.seed, model=args.model,\n",
    "    seqlen=model.seqlen, eval_mode=False\n",
    ")\n",
    "\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "label_smoother = LabelSmoother(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm.tqdm(range(0 + 1, 100 + 1), desc=\"Training progress\",\n",
    "            total=100, dynamic_ncols=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_stack = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straightforward ortho reg w/ SFTT\n",
    "# for iteration in pbar:\n",
    "#     if not idx_stack:\n",
    "#         idx_stack = list(range(0, len(trainloader)))\n",
    "    \n",
    "#     idx = idx_stack.pop(randint(0, len(idx_stack) - 1))\n",
    "    \n",
    "#     data = trainloader[idx]\n",
    "    \n",
    "#     input = data[0]\n",
    "#     target = data[1]\n",
    "    \n",
    "#     output = model(input)\n",
    "    \n",
    "#     loss = label_smoother(output, input, shift_labels=True)\n",
    "#     sym = torch.mm(Q, torch.t(Q))\n",
    "#     sym -= torch.eye(Q.shape[0]).to(sym.device)\n",
    "#     # ls_ort = sym.abs().sum()   # poor match to geometry of orthogonal matrices\n",
    "#     ortho_reg = sym.pow(2.0).sum()\n",
    "#     loss = loss + 10.0 * ortho_reg\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         pbar.set_postfix(\n",
    "#             {'CE': f'{loss.item():.3f}',\n",
    "#             'Ortho': f'{ortho_reg.item():.3f}',\n",
    "#             'det(Q)': f'{torch.linalg.det(Q):.3f}'}\n",
    "#         )\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "# torch.save(Q, 'Q_Hreg1k.pt')\n",
    "# Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-forward hook-style test\n",
    "# q = random_orthogonal_matrix(2, device).to(dtype=torch.float32)\n",
    "\n",
    "# Q = nn.Parameter(q, requires_grad=True)\n",
    "\n",
    "# print('Q', Q)\n",
    "\n",
    "# layer = nn.Linear(2, 2, False).to(device)\n",
    "\n",
    "# for p in layer.parameters():\n",
    "#     p.requires_grad = False\n",
    "    \n",
    "# layer.register_buffer('Q', Q)\n",
    "\n",
    "# def hook(module, input):\n",
    "#     x = F.linear(input[0], module.Q)\n",
    "#     return (x,) + input[1:]\n",
    "\n",
    "# layer.register_forward_pre_hook(hook)\n",
    "\n",
    "# x = torch.eye(2).to(device)\n",
    "# target = torch.eye(2).to(device)\n",
    "\n",
    "# optim = torch.optim.Adam([Q], lr=1e-2)\n",
    "\n",
    "# for i in range(1000000):\n",
    "#     y = layer(x)\n",
    "#     loss = F.mse_loss(y, target)\n",
    "    \n",
    "#     sym = torch.mm(Q, torch.t(Q))\n",
    "#     sym -= torch.eye(Q.shape[0]).to(device)\n",
    "#     # ls_ort = sym.abs().sum()   # poor match to geometry of orthogonal matrices\n",
    "#     ortho_reg = sym.pow(2.0).sum()\n",
    "    \n",
    "#     loss = loss + 0.1 * ortho_reg\n",
    "    \n",
    "#     optim.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optim.step()\n",
    "    \n",
    "#     if i % 10000 == 0:\n",
    "#         #print(\"-----W-----\")\n",
    "#         #print(layer.weight)\n",
    "#         print(\"-----Q-----\")\n",
    "#         print(Q)\n",
    "#         with torch.no_grad():\n",
    "#             print(torch.linalg.det(Q).item(), ortho_reg.item())\n",
    "#         print(\"-----Y-----\")\n",
    "#         print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = torch.zeros_like(Q1).requires_grad_(False)\n",
    "M2s = []\n",
    "\n",
    "for Q2 in Q2s:\n",
    "    M2 = torch.zeros_like(Q2).requires_grad_(False)\n",
    "    M2s.append(M2)\n",
    "\n",
    "beta = 0.9\n",
    "epsilon = 1e-8\n",
    "s = 5\n",
    "\n",
    "@torch.no_grad()\n",
    "def cayley_sgd(X, M, l, beta, epsilon, q, s):\n",
    "    if X.grad is not None:\n",
    "        M = beta * M - X.grad\n",
    "        #print('M', M.isnan().any().item())\n",
    "        MK = torch.matmul(M, X.T)\n",
    "        W_hat = MK - 0.5 * torch.matmul(X, torch.matmul(X.T, MK))\n",
    "        #print('W_hat', W_hat.isnan().any().item())\n",
    "        W = W_hat - W_hat.T\n",
    "        \n",
    "        M = torch.matmul(W, X)\n",
    "        \n",
    "        alpha = min(l, 2. * q / (torch.norm(W) + epsilon))\n",
    "        #print('alpha', alpha)\n",
    "        Y = X + alpha * M\n",
    "        \n",
    "        for i in range(s):\n",
    "            Y = X + alpha / 2 * torch.matmul(W, X + Y)\n",
    "        \n",
    "        X.data = Y\n",
    "        X.grad.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(iter, total_iter, max_lr, min_lr):\n",
    "    return max_lr - iter / total_iter * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|          | 0/100 [05:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m label_smoother(output, \u001b[38;5;28minput\u001b[39m, shift_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#Q1.retain_grad()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#for Q2 in Q2s:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#    Q2.retain_grad()\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m lr \u001b[38;5;241m=\u001b[39m lr_schedule(iteration, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(iteration, output['logits'].isnan().any().item(), loss.isnan().any().item(), lr)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:503\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    495\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    496\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    502\u001b[0m     )\n\u001b[0;32m--> 503\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:254\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "for iteration in pbar:\n",
    "    if not idx_stack:\n",
    "        idx_stack = list(range(0, len(trainloader)))\n",
    "    \n",
    "    idx = idx_stack.pop(randint(0, len(idx_stack) - 1))\n",
    "    \n",
    "    data = trainloader[idx]\n",
    "    \n",
    "    input = data[0]\n",
    "    target = data[1]\n",
    "    #print()\n",
    "    output = model(input)\n",
    "    \n",
    "    loss = label_smoother(output, input, shift_labels=True)\n",
    "    \n",
    "    #Q1.retain_grad()\n",
    "    #for Q2 in Q2s:\n",
    "    #    Q2.retain_grad()\n",
    "    loss.backward()\n",
    "    lr = lr_schedule(iteration, 100, 0.01, 0)\n",
    "    #print(iteration, output['logits'].isnan().any().item(), loss.isnan().any().item(), lr)\n",
    "    cayley_sgd(Q1, M1, lr, 0.9, 1e-8, 0.5, 2)\n",
    "    #count = 0\n",
    "    for Q2, M2 in zip(Q2s, M2s):\n",
    "        cayley_sgd(Q2, M2, lr, 0.9, 1e-8, 0.5, 2)\n",
    "        #if Q2.grad is not None:\n",
    "        #    count += 1\n",
    "    #print(iteration, count)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar.set_postfix(\n",
    "            {'CE': f'{loss.item():.3f}',\n",
    "             'ortho1': f'{(torch.matmul(Q1, Q1.T) - torch.eye(Q1.size(0)).to(Q1.device)).sum().item():.3f}',\n",
    "             'ortho2': f'{(torch.matmul(Q2, Q2.T) - torch.eye(Q2.size(0)).to(Q2.device)).sum().item():.3f}',\n",
    "             #'det(Q)': f'{torch.linalg.det(Q):.3f}'\n",
    "             }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0156, -0.0157, -0.0156,  ..., -0.0156, -0.0156, -0.0156],\n",
       "        [ 0.0157, -0.0157,  0.0157,  ..., -0.0157,  0.0156, -0.0156],\n",
       "        [ 0.0156,  0.0157, -0.0156,  ...,  0.0156, -0.0156, -0.0156],\n",
       "        ...,\n",
       "        [-0.0156,  0.0157, -0.0156,  ...,  0.0156, -0.0156,  0.0156],\n",
       "        [-0.0156, -0.0156,  0.0156,  ..., -0.0156,  0.0156,  0.0156],\n",
       "        [-0.0156,  0.0156,  0.0156,  ...,  0.0156,  0.0156, -0.0156]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0884, -0.0884, -0.0884,  ..., -0.0884, -0.0884, -0.0884],\n",
       "        [ 0.0884, -0.0884,  0.0884,  ..., -0.0884,  0.0884, -0.0884],\n",
       "        [ 0.0884,  0.0884, -0.0884,  ...,  0.0884, -0.0884, -0.0884],\n",
       "        ...,\n",
       "        [-0.0884,  0.0884, -0.0884,  ..., -0.0884,  0.0884, -0.0884],\n",
       "        [ 0.0884,  0.0884, -0.0884,  ..., -0.0884,  0.0884,  0.0884],\n",
       "        [-0.0884,  0.0884,  0.0884,  ..., -0.0884, -0.0884,  0.0884]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Q1, 'Q1.pt')\n",
    "torch.save(Q2, 'Q2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mhead_dim\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.layers[0].self_attn.head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
